{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wanghaiy2018/AC0499_2024/blob/main/Installation/Ollama_sample12-14.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6cbc2026-6598-47fe-ade3-82a6749711f1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cbc2026-6598-47fe-ade3-82a6749711f1",
        "outputId": "5d1661ec-4fe9-46ec-ff32-48fb5e18312a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: {'model': 'llama3.2', 'created_at': '2024-12-16T18:43:10.7731551Z', 'response': \"The quadratic formula is:\\n\\nx = (-b ± √(b^2 - 4ac)) / 2a\\n\\nWhere a, b, and c are the coefficients of the quadratic equation in the form ax^2 + bx + c = 0.\\n\\nHere's how it works:\\n\\n1. Plug in the values of a, b, and c into the formula.\\n2. Calculate the value inside the square root (b^2 - 4ac).\\n3. Simplify the expression to get two possible solutions for x.\\n4. Choose the correct solution based on the context of the problem.\\n\\nExample:\\n\\nSuppose we have the quadratic equation x^2 + 5x + 6 = 0\\n\\nIn this case, a = 1, b = 5, and c = 6.\\n\\nUsing the quadratic formula, we get:\\n\\nx = (-5 ± √(5^2 - 4*1*6)) / 2*1\\nx = (-5 ± √(25 - 24)) / 2\\nx = (-5 ± √1) / 2\\n\\nSimplifying further, we get two possible solutions:\\n\\nx = (-5 + 1) / 2 = -2\\nx = (-5 - 1) / 2 = -3\\n\\nTherefore, the solutions to the equation x^2 + 5x + 6 = 0 are x = -2 and x = -3.\\n\\nNote that the quadratic formula will always give you two solutions, but in many cases, one of these solutions may be extraneous or invalid. It's always a good idea to check your solutions by plugging them back into the original equation to ensure they're correct!\", 'done': True, 'done_reason': 'stop', 'context': [128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 271, 128009, 128006, 882, 128007, 271, 10590, 220, 42676, 81, 780, 15150, 128009, 128006, 78191, 128007, 271, 791, 80251, 15150, 374, 1473, 87, 284, 10505, 65, 20903, 122371, 7, 65, 61, 17, 482, 220, 19, 582, 595, 611, 220, 17, 64, 271, 9241, 264, 11, 293, 11, 323, 272, 527, 279, 37929, 315, 279, 80251, 24524, 304, 279, 1376, 3944, 61, 17, 489, 45341, 489, 272, 284, 220, 15, 382, 8586, 596, 1268, 433, 4375, 1473, 16, 13, 53909, 304, 279, 2819, 315, 264, 11, 293, 11, 323, 272, 1139, 279, 15150, 627, 17, 13, 21157, 279, 907, 4871, 279, 9518, 3789, 320, 65, 61, 17, 482, 220, 19, 582, 4390, 18, 13, 62342, 1463, 279, 7645, 311, 636, 1403, 3284, 10105, 369, 865, 627, 19, 13, 22991, 279, 4495, 6425, 3196, 389, 279, 2317, 315, 279, 3575, 382, 13617, 1473, 10254, 2972, 584, 617, 279, 80251, 24524, 865, 61, 17, 489, 220, 20, 87, 489, 220, 21, 284, 220, 15, 271, 644, 420, 1162, 11, 264, 284, 220, 16, 11, 293, 284, 220, 20, 11, 323, 272, 284, 220, 21, 382, 16834, 279, 80251, 15150, 11, 584, 636, 1473, 87, 284, 10505, 20, 20903, 122371, 7, 20, 61, 17, 482, 220, 19, 9, 16, 9, 21, 595, 611, 220, 17, 9, 16, 198, 87, 284, 10505, 20, 20903, 122371, 7, 914, 482, 220, 1187, 595, 611, 220, 17, 198, 87, 284, 10505, 20, 20903, 122371, 16, 8, 611, 220, 17, 271, 50, 6517, 7922, 4726, 11, 584, 636, 1403, 3284, 10105, 1473, 87, 284, 10505, 20, 489, 220, 16, 8, 611, 220, 17, 284, 482, 17, 198, 87, 284, 10505, 20, 482, 220, 16, 8, 611, 220, 17, 284, 482, 18, 271, 55915, 11, 279, 10105, 311, 279, 24524, 865, 61, 17, 489, 220, 20, 87, 489, 220, 21, 284, 220, 15, 527, 865, 284, 482, 17, 323, 865, 284, 482, 18, 382, 9290, 430, 279, 80251, 15150, 690, 2744, 3041, 499, 1403, 10105, 11, 719, 304, 1690, 5157, 11, 832, 315, 1521, 10105, 1253, 387, 11741, 18133, 477, 8482, 13, 1102, 596, 2744, 264, 1695, 4623, 311, 1817, 701, 10105, 555, 628, 36368, 1124, 1203, 1139, 279, 4113, 24524, 311, 6106, 814, 2351, 4495, 0], 'total_duration': 7672370000, 'load_duration': 5199468800, 'prompt_eval_count': 31, 'prompt_eval_duration': 107000000, 'eval_count': 351, 'eval_duration': 2364000000}\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# Define the endpoint and payload\n",
        "\n",
        "\n",
        "myip=\"http://localhost\"\n",
        "\n",
        "url = myip+\":11434/api/generate\"\n",
        "payload = {\n",
        "    \"model\": \"llama3.2\",\n",
        "#    \"model\": \"llama3.3\",\n",
        "#    \"model\": \"qwq\",\n",
        "#    \"model\":  \"hf.co/bartowski/Llama-3.2-3B-Instruct-GGUF\",\n",
        "\n",
        "    \"prompt\": \"math  quandratic formula\",\n",
        "    \"stream\": False\n",
        "}\n",
        "headers = {\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "# Send the POST request\n",
        "try:\n",
        "    response = requests.post(url, headers=headers, json=payload, timeout=30000)\n",
        "    response.raise_for_status()  # Raise an HTTPError for bad responses\n",
        "    print(\"Response:\", response.json())\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "20c35031-f30e-4d3e-9fd8-cb62d06c1515",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20c35031-f30e-4d3e-9fd8-cb62d06c1515",
        "outputId": "fd0734a8-b6c4-4bdb-be3e-4961a365e1d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw Llama Response: Based on the given causal relationships, we can determine that A does not directly affect C.\n",
            "\n",
            "The relationship between A and C is indirect. A causes B, and then B causes C. However, it's specified that A is independent of C given B, which means that knowing whether B has occurred (i.e., knowing whether A caused B) does not provide any additional information about the probability or effect of A on C.\n",
            "\n",
            "In other words, even if we know that A caused B, we cannot conclude that this will lead to an effect on C. The relationship between A and C is conditioned on the occurrence of B, so it's mediated by B, rather than being a direct cause-and-effect relationship between A and C.\n",
            "State: initial_state, Action: achieve_goal, Result: A does not directly affect C, Attempts: 1\n",
            "Goal achieved with result: A does not directly affect C\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# Class to interact with the Llama model server\n",
        "class LlamaLLM:\n",
        "    def __init__(self, url):\n",
        "        self.url = url\n",
        "\n",
        "    def generate(self, prompt):\n",
        "        payload = {\n",
        "            \"model\": \"llama3.2\",\n",
        "            \"prompt\": prompt,\n",
        "            \"stream\": False\n",
        "        }\n",
        "        headers = {\n",
        "            'Content-Type': 'application/json'\n",
        "        }\n",
        "        response = requests.post(self.url, headers=headers, data=json.dumps(payload),timeout=30000)\n",
        "        response.raise_for_status()\n",
        "        return response.json()['response']\n",
        "\n",
        "# Environment class remains the same\n",
        "class Environment:\n",
        "    def __init__(self):\n",
        "        self.state = 'initial_state'  # Initial state\n",
        "        self.goal_achieved = False  # Goal not achieved\n",
        "        self.result = None  # Store result\n",
        "\n",
        "    def get_state(self):\n",
        "        return self.state  # Get current state\n",
        "\n",
        "    def change_state(self, result):\n",
        "        self.state = 'goal_state'  # Change to goal state\n",
        "        self.result = result  # Store result\n",
        "\n",
        "# ReActAgent class modified to use LlamaLLM for reasoning\n",
        "class ReActAgent:\n",
        "    def __init__(self, environment, llm, max_attempts=5):\n",
        "        self.environment = environment  # Initialize environment\n",
        "        self.llm = llm  # Llama 3.1 model\n",
        "        self.max_attempts = max_attempts  # Max attempts\n",
        "\n",
        "    def perceive(self):\n",
        "        return self.environment.get_state()  # Perceive environment state\n",
        "\n",
        "    def reason(self, state):\n",
        "        # Causal reasoning prompt based on paper example\n",
        "        prompt = \"\"\"\n",
        "        Given the following causal relationships:\n",
        "        - A causes B\n",
        "        - B causes C\n",
        "        - A is independent of C given B\n",
        "\n",
        "        Question: Does A directly affect C?\n",
        "        \"\"\"\n",
        "        response = self.llm.generate(prompt).strip()\n",
        "        print(f\"Raw Llama Response: {response}\")  # Debugging: print raw response\n",
        "        if 'No' in response or 'independent' in response:\n",
        "            return 'achieve_goal', \"A does not directly affect C\"\n",
        "        return 'take_action', None\n",
        "\n",
        "    def act(self, action, result):\n",
        "        if action == 'achieve_goal':\n",
        "            self.environment.goal_achieved = True  # Achieve goal\n",
        "            self.environment.change_state(result)  # Change state and store result\n",
        "\n",
        "# Initialize the Llama model with the server URL\n",
        "\n",
        "\n",
        "myip=\"http://localhost\"\n",
        "\n",
        "llm = LlamaLLM(url=myip+\":11434/api/generate\")\n",
        "\n",
        "# Create environment and agent\n",
        "env = Environment()\n",
        "agent = ReActAgent(env, llm)\n",
        "\n",
        "# Agent perception, reasoning, and action loop\n",
        "attempts = 0\n",
        "while not env.goal_achieved and attempts < agent.max_attempts:\n",
        "    state = agent.perceive()\n",
        "    action, result = agent.reason(state)\n",
        "    agent.act(action, result)\n",
        "    attempts += 1\n",
        "    print(f\"State: {state}, Action: {action}, Result: {result}, Attempts: {attempts}\")\n",
        "\n",
        "if env.goal_achieved:\n",
        "    print(f\"Goal achieved with result: {env.result}\")\n",
        "else:\n",
        "    print(\"Failed to achieve goal within the maximum number of attempts.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e94748fc-7671-42e1-a02a-4c62f5acee64",
      "metadata": {
        "id": "e94748fc-7671-42e1-a02a-4c62f5acee64"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1966602-d2f3-402f-bcbf-db1914593fe4",
      "metadata": {
        "id": "d1966602-d2f3-402f-bcbf-db1914593fe4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}