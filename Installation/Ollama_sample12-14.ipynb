{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DynamicLLM/LLM2024/blob/main/Installation/Ollama_sample12-14.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6cbc2026-6598-47fe-ade3-82a6749711f1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cbc2026-6598-47fe-ade3-82a6749711f1",
        "outputId": "f53fc76b-f08a-4cce-9566-2413e51c4efd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: {'model': 'nezahatkorkmaz/deepseek-v3', 'created_at': '2024-12-31T19:02:42.3243377Z', 'response': 'DeepSeek is an open-source, highly customizable, and performance-optimized search library developed by Google. It\\'s primarily used for full-text search in large datasets, such as those found in big data platforms like Apache HBase, Apache Cassandra, and Apache Solr.\\n\\n**Features of DeepSeek v3:**\\n\\n1.  **High-performance search**: DeepSeek is designed to provide fast and efficient search results, even on massive datasets.\\n2.  **Flexible schema design**: It allows for a flexible schema design, enabling users to adapt to changing data structures without modifying the underlying codebase.\\n3.  **Support for various indexing formats**: DeepSeek supports multiple indexing formats, including BRIEF, BRTF8, and JSON.\\n\\n**How DeepSeek v3 Works:**\\n\\n1.  **Text processing**: DeepSeek reads in text data from a source (e.g., a database or file) and applies tokenization, stemming, and lemmatization.\\n2.  **Indexing**: It generates an index of the processed text data using the chosen indexing format.\\n3.  **Search**: During search queries, DeepSeek uses the generated index to quickly locate matching documents.\\n\\n**DeepSeek v3 Use Cases:**\\n\\n1.  **Full-text search in big data platforms**: DeepSeek is suitable for large-scale applications that require fast and efficient full-text search capabilities.\\n2.  **Data analytics and reporting**: It can be used in data analytics and reporting scenarios where rapid text processing and searching are essential.\\n\\n**Example Code Snippet:**\\n\\nHere\\'s a simplified example of how to use DeepSeek v3 in Python:\\n\\n```python\\nimport deepseek\\n\\n# Initialize the search engine with an index file\\nengine = deepseek.DeepSeek(\"index.bri\")\\n\\n# Create a new document with some text content\\ndoc = {\\n    \"content\": \"This is a sample document.\",\\n}\\n\\n# Add the document to the index\\nengine.add_document(doc)\\n\\n# Search for documents containing specific keywords\\nresults = engine.search(\"sample\", num_results=2)\\n```\\n\\nIn this example, we initialize the DeepSeek search engine with an index file and create a new document with some text content. We then add the document to the index and perform a search query using the `search` method.\\n\\nBy leveraging the capabilities of DeepSeek v3, developers can build high-performance full-text search applications that scale efficiently across large datasets.', 'done': True, 'done_reason': 'stop', 'context': [128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 271, 2675, 527, 264, 8147, 18328, 8405, 18682, 40450, 15293, 311, 11886, 6485, 11058, 9256, 13, 128009, 128006, 882, 128007, 271, 5269, 374, 18682, 40450, 85, 18, 128009, 128006, 78191, 128007, 271, 34564, 40450, 374, 459, 1825, 31874, 11, 7701, 63174, 11, 323, 5178, 12, 99968, 2778, 6875, 8040, 555, 5195, 13, 1102, 596, 15871, 1511, 369, 2539, 9529, 2778, 304, 3544, 30525, 11, 1778, 439, 1884, 1766, 304, 2466, 828, 15771, 1093, 9091, 473, 4066, 11, 9091, 82342, 11, 323, 9091, 11730, 81, 382, 334, 22043, 315, 18682, 40450, 348, 18, 25, 57277, 16, 13, 220, 3146, 12243, 58574, 2778, 96618, 18682, 40450, 374, 6319, 311, 3493, 5043, 323, 11297, 2778, 3135, 11, 1524, 389, 11191, 30525, 627, 17, 13, 220, 3146, 76247, 11036, 2955, 96618, 1102, 6276, 369, 264, 19303, 11036, 2955, 11, 28462, 3932, 311, 10737, 311, 10223, 828, 14726, 2085, 47141, 279, 16940, 2082, 3231, 627, 18, 13, 220, 3146, 8075, 369, 5370, 53080, 20447, 96618, 18682, 40450, 11815, 5361, 53080, 20447, 11, 2737, 426, 4403, 15229, 11, 426, 5463, 37, 23, 11, 323, 4823, 382, 334, 4438, 18682, 40450, 348, 18, 21785, 25, 57277, 16, 13, 220, 3146, 1199, 8863, 96618, 18682, 40450, 16181, 304, 1495, 828, 505, 264, 2592, 320, 68, 1326, 2637, 264, 4729, 477, 1052, 8, 323, 17208, 4037, 2065, 11, 77044, 11, 323, 514, 3906, 266, 2065, 627, 17, 13, 220, 3146, 1581, 287, 96618, 1102, 27983, 459, 1963, 315, 279, 15590, 1495, 828, 1701, 279, 12146, 53080, 3645, 627, 18, 13, 220, 3146, 6014, 96618, 12220, 2778, 20126, 11, 18682, 40450, 5829, 279, 8066, 1963, 311, 6288, 25539, 12864, 9477, 382, 334, 34564, 40450, 348, 18, 5560, 47124, 25, 57277, 16, 13, 220, 3146, 9619, 9529, 2778, 304, 2466, 828, 15771, 96618, 18682, 40450, 374, 14791, 369, 3544, 13230, 8522, 430, 1397, 5043, 323, 11297, 2539, 9529, 2778, 17357, 627, 17, 13, 220, 3146, 1061, 28975, 323, 13122, 96618, 1102, 649, 387, 1511, 304, 828, 28975, 323, 13122, 26350, 1405, 11295, 1495, 8863, 323, 15389, 527, 7718, 382, 334, 13617, 6247, 13358, 22309, 25, 57277, 8586, 596, 264, 44899, 3187, 315, 1268, 311, 1005, 18682, 40450, 348, 18, 304, 13325, 1473, 74694, 12958, 198, 475, 5655, 26797, 271, 2, 9185, 279, 2778, 4817, 449, 459, 1963, 1052, 198, 8680, 284, 5655, 26797, 56702, 40450, 446, 1275, 960, 462, 5240, 2, 4324, 264, 502, 2246, 449, 1063, 1495, 2262, 198, 5349, 284, 341, 262, 330, 1834, 794, 330, 2028, 374, 264, 6205, 2246, 10560, 633, 2, 2758, 279, 2246, 311, 279, 1963, 198, 8680, 1388, 27326, 19702, 696, 2, 7694, 369, 9477, 8649, 3230, 21513, 198, 8234, 284, 4817, 9472, 446, 13925, 498, 1661, 13888, 28, 17, 340, 14196, 19884, 644, 420, 3187, 11, 584, 9656, 279, 18682, 40450, 2778, 4817, 449, 459, 1963, 1052, 323, 1893, 264, 502, 2246, 449, 1063, 1495, 2262, 13, 1226, 1243, 923, 279, 2246, 311, 279, 1963, 323, 2804, 264, 2778, 3319, 1701, 279, 1595, 1874, 63, 1749, 382, 1383, 77582, 279, 17357, 315, 18682, 40450, 348, 18, 11, 13707, 649, 1977, 1579, 58574, 2539, 9529, 2778, 8522, 430, 5569, 30820, 4028, 3544, 30525, 13], 'total_duration': 3672137700, 'load_duration': 29249700, 'prompt_eval_count': 46, 'prompt_eval_duration': 35000000, 'eval_count': 489, 'eval_duration': 3607000000}\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# Define the endpoint and payload\n",
        "\n",
        "\n",
        "myip=\"http://localhost\"\n",
        "\n",
        "url = myip+\":11434/api/generate\"\n",
        "payload = {\n",
        "#    \"model\": \"llama3.2\",\n",
        "     \"model\": \"nezahatkorkmaz/deepseek-v3\",\n",
        "#    \"model\": \"llama3.3\",\n",
        "#    \"model\": \"qwq\",\n",
        "#    \"model\":  \"hf.co/bartowski/Llama-3.2-3B-Instruct-GGUF\",\n",
        "#\n",
        "    \"prompt\": \"how is DeepSeekv3\",\n",
        "    \"stream\": False\n",
        "}\n",
        "headers = {\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "# Send the POST request\n",
        "try:\n",
        "    response = requests.post(url, headers=headers, json=payload, timeout=30000)\n",
        "    response.raise_for_status()  # Raise an HTTPError for bad responses\n",
        "    print(\"Response:\", response.json())\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20c35031-f30e-4d3e-9fd8-cb62d06c1515",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20c35031-f30e-4d3e-9fd8-cb62d06c1515",
        "outputId": "fd0734a8-b6c4-4bdb-be3e-4961a365e1d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw Llama Response: Based on the given causal relationships, we can determine that A does not directly affect C.\n",
            "\n",
            "The relationship between A and C is indirect. A causes B, and then B causes C. However, it's specified that A is independent of C given B, which means that knowing whether B has occurred (i.e., knowing whether A caused B) does not provide any additional information about the probability or effect of A on C.\n",
            "\n",
            "In other words, even if we know that A caused B, we cannot conclude that this will lead to an effect on C. The relationship between A and C is conditioned on the occurrence of B, so it's mediated by B, rather than being a direct cause-and-effect relationship between A and C.\n",
            "State: initial_state, Action: achieve_goal, Result: A does not directly affect C, Attempts: 1\n",
            "Goal achieved with result: A does not directly affect C\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# Class to interact with the Llama model server\n",
        "class LlamaLLM:\n",
        "    def __init__(self, url):\n",
        "        self.url = url\n",
        "\n",
        "    def generate(self, prompt):\n",
        "        payload = {\n",
        "            \"model\": \"llama3.2\",\n",
        "            \"prompt\": prompt,\n",
        "            \"stream\": False\n",
        "        }\n",
        "        headers = {\n",
        "            'Content-Type': 'application/json'\n",
        "        }\n",
        "        response = requests.post(self.url, headers=headers, data=json.dumps(payload),timeout=30000)\n",
        "        response.raise_for_status()\n",
        "        return response.json()['response']\n",
        "\n",
        "# Environment class remains the same\n",
        "class Environment:\n",
        "    def __init__(self):\n",
        "        self.state = 'initial_state'  # Initial state\n",
        "        self.goal_achieved = False  # Goal not achieved\n",
        "        self.result = None  # Store result\n",
        "\n",
        "    def get_state(self):\n",
        "        return self.state  # Get current state\n",
        "\n",
        "    def change_state(self, result):\n",
        "        self.state = 'goal_state'  # Change to goal state\n",
        "        self.result = result  # Store result\n",
        "\n",
        "# ReActAgent class modified to use LlamaLLM for reasoning\n",
        "class ReActAgent:\n",
        "    def __init__(self, environment, llm, max_attempts=5):\n",
        "        self.environment = environment  # Initialize environment\n",
        "        self.llm = llm  # Llama 3.1 model\n",
        "        self.max_attempts = max_attempts  # Max attempts\n",
        "\n",
        "    def perceive(self):\n",
        "        return self.environment.get_state()  # Perceive environment state\n",
        "\n",
        "    def reason(self, state):\n",
        "        # Causal reasoning prompt based on paper example\n",
        "        prompt = \"\"\"\n",
        "        Given the following causal relationships:\n",
        "        - A causes B\n",
        "        - B causes C\n",
        "        - A is independent of C given B\n",
        "\n",
        "        Question: Does A directly affect C?\n",
        "        \"\"\"\n",
        "        response = self.llm.generate(prompt).strip()\n",
        "        print(f\"Raw Llama Response: {response}\")  # Debugging: print raw response\n",
        "        if 'No' in response or 'independent' in response:\n",
        "            return 'achieve_goal', \"A does not directly affect C\"\n",
        "        return 'take_action', None\n",
        "\n",
        "    def act(self, action, result):\n",
        "        if action == 'achieve_goal':\n",
        "            self.environment.goal_achieved = True  # Achieve goal\n",
        "            self.environment.change_state(result)  # Change state and store result\n",
        "\n",
        "# Initialize the Llama model with the server URL\n",
        "\n",
        "\n",
        "myip=\"http://localhost\"\n",
        "\n",
        "llm = LlamaLLM(url=myip+\":11434/api/generate\")\n",
        "\n",
        "# Create environment and agent\n",
        "env = Environment()\n",
        "agent = ReActAgent(env, llm)\n",
        "\n",
        "# Agent perception, reasoning, and action loop\n",
        "attempts = 0\n",
        "while not env.goal_achieved and attempts < agent.max_attempts:\n",
        "    state = agent.perceive()\n",
        "    action, result = agent.reason(state)\n",
        "    agent.act(action, result)\n",
        "    attempts += 1\n",
        "    print(f\"State: {state}, Action: {action}, Result: {result}, Attempts: {attempts}\")\n",
        "\n",
        "if env.goal_achieved:\n",
        "    print(f\"Goal achieved with result: {env.result}\")\n",
        "else:\n",
        "    print(\"Failed to achieve goal within the maximum number of attempts.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e94748fc-7671-42e1-a02a-4c62f5acee64",
      "metadata": {
        "id": "e94748fc-7671-42e1-a02a-4c62f5acee64"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1966602-d2f3-402f-bcbf-db1914593fe4",
      "metadata": {
        "id": "d1966602-d2f3-402f-bcbf-db1914593fe4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}