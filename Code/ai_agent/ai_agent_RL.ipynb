{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac6c2964-bf5a-4bed-a3fb-75a150852bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'llama3.1', 'created_at': '2024-07-31T13:24:40.1172474Z', 'response': 'As of my last update in April 2023, the President of the United States is Joe Biden. He took office on January 20, 2021, succeeding Donald Trump. Prior to becoming President, Joe Biden served as Vice President under Barack Obama from 2009 to 2017.', 'done': True, 'done_reason': 'stop', 'context': [128006, 882, 128007, 271, 14965, 374, 279, 4872, 315, 7427, 128009, 128006, 78191, 128007, 271, 2170, 315, 856, 1566, 2713, 304, 5936, 220, 2366, 18, 11, 279, 4900, 315, 279, 3723, 4273, 374, 13142, 38180, 13, 1283, 3952, 5274, 389, 6186, 220, 508, 11, 220, 2366, 16, 11, 73820, 9641, 3420, 13, 32499, 311, 10671, 4900, 11, 13142, 38180, 10434, 439, 23270, 4900, 1234, 24448, 7250, 505, 220, 1049, 24, 311, 220, 679, 22, 13], 'total_duration': 51249649700, 'load_duration': 21295352300, 'prompt_eval_count': 16, 'prompt_eval_duration': 3972117000, 'eval_count': 61, 'eval_duration': 25964875000}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Define the URL and the data payload\n",
    "url = \"http://localhost:11434/api/generate\"\n",
    "payload = {\n",
    "    \"model\": \"llama3.1\",\n",
    "    \"prompt\": \"who is the president of USA\",\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "# Set the headers\n",
    "headers = {\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "# Make the POST request\n",
    "response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
    "\n",
    "# Print the response\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e205a953-5558-442d-9c1a-1eabb5a15c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Llama Response: Based on the information provided:\n",
      "\n",
      "1. A causes B (A → B)\n",
      "2. B causes C (B → C)\n",
      "3. A is independent of C given B (A ⊥ C | B)\n",
      "\n",
      "The question asks whether A directly affects C.\n",
      "\n",
      "Since A is independent of C given B, this implies that any effect of A on C must be mediated through B. In other words, the relationship between A and C is indirect, not direct.\n",
      "\n",
      "Therefore, the answer is: **No**, A does not directly affect C.\n",
      "State: initial_state, Action: achieve_goal, Result: A does not directly affect C, Attempts: 1\n",
      "Goal achieved with result: A does not directly affect C\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Class to interact with the Llama model server\n",
    "class LlamaLLM:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "\n",
    "    def generate(self, prompt):\n",
    "        payload = {\n",
    "            \"model\": \"llama3.1\",\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False\n",
    "        }\n",
    "        headers = {\n",
    "            'Content-Type': 'application/json'\n",
    "        }\n",
    "        response = requests.post(self.url, headers=headers, data=json.dumps(payload))\n",
    "        response.raise_for_status()\n",
    "        return response.json()['response']\n",
    "\n",
    "# Environment class remains the same\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.state = 'initial_state'  # Initial state\n",
    "        self.goal_achieved = False  # Goal not achieved\n",
    "        self.result = None  # Store result\n",
    "\n",
    "    def get_state(self):\n",
    "        return self.state  # Get current state\n",
    "\n",
    "    def change_state(self, result):\n",
    "        self.state = 'goal_state'  # Change to goal state\n",
    "        self.result = result  # Store result\n",
    "\n",
    "# ReActAgent class modified to use LlamaLLM for reasoning\n",
    "class ReActAgent:\n",
    "    def __init__(self, environment, llm, max_attempts=5):\n",
    "        self.environment = environment  # Initialize environment\n",
    "        self.llm = llm  # Llama 3.1 model\n",
    "        self.max_attempts = max_attempts  # Max attempts\n",
    "\n",
    "    def perceive(self):\n",
    "        return self.environment.get_state()  # Perceive environment state\n",
    "\n",
    "    def reason(self, state):\n",
    "        # Causal reasoning prompt based on paper example\n",
    "        prompt = \"\"\"\n",
    "        Given the following causal relationships:\n",
    "        - A causes B\n",
    "        - B causes C\n",
    "        - A is independent of C given B\n",
    "        \n",
    "        Question: Does A directly affect C?\n",
    "        \"\"\"\n",
    "        response = self.llm.generate(prompt).strip()\n",
    "        print(f\"Raw Llama Response: {response}\")  # Debugging: print raw response\n",
    "        if 'No' in response or 'independent' in response:\n",
    "            return 'achieve_goal', \"A does not directly affect C\"\n",
    "        return 'take_action', None\n",
    "\n",
    "    def act(self, action, result):\n",
    "        if action == 'achieve_goal':\n",
    "            self.environment.goal_achieved = True  # Achieve goal\n",
    "            self.environment.change_state(result)  # Change state and store result\n",
    "\n",
    "# Initialize the Llama model with the server URL\n",
    "llm = LlamaLLM(url=\"http://localhost:11434/api/generate\")\n",
    "\n",
    "# Create environment and agent\n",
    "env = Environment()\n",
    "agent = ReActAgent(env, llm)\n",
    "\n",
    "# Agent perception, reasoning, and action loop\n",
    "attempts = 0\n",
    "while not env.goal_achieved and attempts < agent.max_attempts:\n",
    "    state = agent.perceive()\n",
    "    action, result = agent.reason(state)\n",
    "    agent.act(action, result)\n",
    "    attempts += 1\n",
    "    print(f\"State: {state}, Action: {action}, Result: {result}, Attempts: {attempts}\")\n",
    "\n",
    "if env.goal_achieved:\n",
    "    print(f\"Goal achieved with result: {env.result}\")\n",
    "else:\n",
    "    print(\"Failed to achieve goal within the maximum number of attempts.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "557a58b7-ef33-4829-8d10-859ea00d8b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Llama Response: A nice example of a directed acyclic graph (DAG)!\n",
      "\n",
      "Let's analyze the relationships:\n",
      "\n",
      "1. Smoking → Lung Cancer (causal relationship)\n",
      "2. Lung Cancer → Mortality (causal relationship)\n",
      "3. Smoking ⊥ Mortality | Lung Cancer (independence statement, meaning Smoking is independent of Mortality given Lung Cancer)\n",
      "\n",
      "The question asks: Does Smoking directly affect Mortality?\n",
      "\n",
      "To answer this, we need to consider the following:\n",
      "\n",
      "* If there were a direct effect from Smoking to Mortality, it would be represented by an edge from Smoking to Mortality.\n",
      "* However, since Lung Cancer lies on the path between Smoking and Mortality (Smoking → Lung Cancer → Mortality), any effect of Smoking on Mortality must go through Lung Cancer.\n",
      "\n",
      "In other words, given that we know there is a causal relationship between Lung Cancer and Mortality, if Smoking influences Mortality, it can only do so through its effect on Lung Cancer. \n",
      "\n",
      "Now, the independence statement tells us that Smoking is independent of Mortality given Lung Cancer. This means that once we condition on Lung Cancer (i.e., knowing whether someone has Lung Cancer or not), the relationship between Smoking and Mortality becomes \"determined\" by Lung Cancer.\n",
      "\n",
      "However, this independence statement does NOT imply that there is no effect from Smoking to Mortality at all!\n",
      "\n",
      "To summarize:\n",
      "\n",
      "* There is a causal relationship between Smoking and Lung Cancer.\n",
      "* There is a causal relationship between Lung Cancer and Mortality.\n",
      "* The relationship between Smoking and Mortality goes through Lung Cancer.\n",
      "* Given the independence statement, we know that once conditioning on Lung Cancer, Smoking does not directly influence Mortality.\n",
      "\n",
      "So, to answer your question:\n",
      "\n",
      "**No**, Smoking does NOT directly affect Mortality.\n",
      "State: initial_state, Action: achieve_goal, Result: Smoking does not directly affect Mortality, Attempts: 1\n",
      "Goal achieved with result: Smoking does not directly affect Mortality\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Class to interact with the Llama model server\n",
    "class LlamaLLM:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "\n",
    "    def generate(self, prompt):\n",
    "        payload = {\n",
    "            \"model\": \"llama3.1\",\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False\n",
    "        }\n",
    "        headers = {\n",
    "            'Content-Type': 'application/json'\n",
    "        }\n",
    "        response = requests.post(self.url, headers=headers, data=json.dumps(payload))\n",
    "        response.raise_for_status()\n",
    "        return response.json()['response']\n",
    "\n",
    "# Environment class remains the same\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.state = 'initial_state'  # Initial state\n",
    "        self.goal_achieved = False  # Goal not achieved\n",
    "        self.result = None  # Store result\n",
    "\n",
    "    def get_state(self):\n",
    "        return self.state  # Get current state\n",
    "\n",
    "    def change_state(self, result):\n",
    "        self.state = 'goal_state'  # Change to goal state\n",
    "        self.result = result  # Store result\n",
    "\n",
    "# ReActAgent class modified to use LlamaLLM for reasoning\n",
    "class ReActAgent:\n",
    "    def __init__(self, environment, llm, max_attempts=5):\n",
    "        self.environment = environment  # Initialize environment\n",
    "        self.llm = llm  # Llama 3.1 model\n",
    "        self.max_attempts = max_attempts  # Max attempts\n",
    "\n",
    "    def perceive(self):\n",
    "        return self.environment.get_state()  # Perceive environment state\n",
    "\n",
    "    def reason(self, state):\n",
    "        # Causal reasoning prompt based on paper example\n",
    "        prompt = \"\"\"\n",
    "        Given the following causal relationships in a healthcare context:\n",
    "        - Smoking causes Lung Cancer\n",
    "        - Lung Cancer causes Mortality\n",
    "        - Smoking is independent of Mortality given Lung Cancer\n",
    "        \n",
    "        Question: Does Smoking directly affect Mortality?\n",
    "        \"\"\"\n",
    "        response = self.llm.generate(prompt).strip()\n",
    "        print(f\"Raw Llama Response: {response}\")  # Debugging: print raw response\n",
    "        if 'No' in response or 'independent' in response:\n",
    "            return 'achieve_goal', \"Smoking does not directly affect Mortality\"\n",
    "        return 'take_action', None\n",
    "\n",
    "    def act(self, action, result):\n",
    "        if action == 'achieve_goal':\n",
    "            self.environment.goal_achieved = True  # Achieve goal\n",
    "            self.environment.change_state(result)  # Change state and store result\n",
    "\n",
    "# Initialize the Llama model with the server URL\n",
    "llm = LlamaLLM(url=\"http://localhost:11434/api/generate\")\n",
    "\n",
    "# Create environment and agent\n",
    "env = Environment()\n",
    "agent = ReActAgent(env, llm)\n",
    "\n",
    "# Agent perception, reasoning, and action loop\n",
    "attempts = 0\n",
    "while not env.goal_achieved and attempts < agent.max_attempts:\n",
    "    state = agent.perceive()\n",
    "    action, result = agent.reason(state)\n",
    "    agent.act(action, result)\n",
    "    attempts += 1\n",
    "    print(f\"State: {state}, Action: {action}, Result: {result}, Attempts: {attempts}\")\n",
    "\n",
    "if env.goal_achieved:\n",
    "    print(f\"Goal achieved with result: {env.result}\")\n",
    "else:\n",
    "    print(\"Failed to achieve goal within the maximum number of attempts.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36a0ef5c-6944-4684-a1b4-ceb6a2ca95fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Llama Response: Based on the given causal relationships, let's analyze the effect of smoking on mortality:\n",
      "\n",
      "1. **Smoking causes Lung Cancer**: This establishes a direct relationship between smoking and lung cancer.\n",
      "2. **Lung Cancer causes Mortality**: This shows that lung cancer has a direct impact on mortality.\n",
      "\n",
      "However, there is also a statement that **Smoking is independent of Mortality given Lung Cancer**. This means that the effect of smoking on mortality cannot be explained by its association with lung cancer; in other words, once we control for (or \"given\") lung cancer, smoking does not have any additional effect on mortality.\n",
      "\n",
      "Now, let's ask: Does Smoking directly affect Mortality?\n",
      "\n",
      "Since there is a direct relationship between Lung Cancer and Mortality, but no independent effect of Smoking on Mortality given Lung Cancer, the correct answer to this question would be **No**. The direct effect of smoking on mortality is fully mediated by its impact on lung cancer.\n",
      "\n",
      "In simpler terms: While smoking causes lung cancer, which in turn causes death, there's no \"beyond-lung-cancer\" effect of smoking that independently impacts mortality. Therefore, the relationship between smoking and mortality can be considered indirect, through the intermediate effect of causing lung cancer.\n",
      "State: initial_state, Action: achieve_goal, Result: Smoking does not directly affect Mortality, Attempts: 1\n",
      "Goal achieved with result: Smoking does not directly affect Mortality\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Class to interact with the Llama model server\n",
    "class LlamaLLM:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "\n",
    "    def generate(self, prompt):\n",
    "        payload = {\n",
    "            \"model\": \"llama3.1\",\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False\n",
    "        }\n",
    "        headers = {\n",
    "            'Content-Type': 'application/json'\n",
    "        }\n",
    "        response = requests.post(self.url, headers=headers, data=json.dumps(payload))\n",
    "        response.raise_for_status()\n",
    "        return response.json()['response']\n",
    "\n",
    "# Environment class remains the same\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.state = 'initial_state'  # Initial state\n",
    "        self.goal_achieved = False  # Goal not achieved\n",
    "        self.result = None  # Store result\n",
    "\n",
    "    def get_state(self):\n",
    "        return self.state  # Get current state\n",
    "\n",
    "    def change_state(self, result):\n",
    "        self.state = 'goal_state'  # Change to goal state\n",
    "        self.result = result  # Store result\n",
    "\n",
    "# ReActAgent class modified to use LlamaLLM for reasoning\n",
    "class ReActAgent:\n",
    "    def __init__(self, environment, llm, max_attempts=5):\n",
    "        self.environment = environment  # Initialize environment\n",
    "        self.llm = llm  # Llama 3.1 model\n",
    "        self.max_attempts = max_attempts  # Max attempts\n",
    "\n",
    "    def perceive(self):\n",
    "        return self.environment.get_state()  # Perceive environment state\n",
    "\n",
    "    def reason(self, state):\n",
    "        # Causal reasoning prompt based on paper example\n",
    "        prompt = \"\"\"\n",
    "        Given the following causal relationships in a healthcare context:\n",
    "        - Smoking causes Lung Cancer\n",
    "        - Lung Cancer causes Mortality\n",
    "        - Smoking is independent of Mortality given Lung Cancer\n",
    "        \n",
    "        Question: Does Smoking directly affect Mortality?\n",
    "        \"\"\"\n",
    "        response = self.llm.generate(prompt).strip()\n",
    "        print(f\"Raw Llama Response: {response}\")  # Debugging: print raw response\n",
    "        if 'No' in response or 'independent' in response:\n",
    "            return 'achieve_goal', \"Smoking does not directly affect Mortality\"\n",
    "        return 'take_action', None\n",
    "\n",
    "    def act(self, action, result):\n",
    "        if action == 'achieve_goal':\n",
    "            self.environment.goal_achieved = True  # Achieve goal\n",
    "            self.environment.change_state(result)  # Change state and store result\n",
    "\n",
    "# Initialize the Llama model with the server URL\n",
    "llm = LlamaLLM(url=\"http://localhost:11434/api/generate\")\n",
    "\n",
    "# Create environment and agent\n",
    "env = Environment()\n",
    "agent = ReActAgent(env, llm)\n",
    "\n",
    "# Agent perception, reasoning, and action loop\n",
    "attempts = 0\n",
    "while not env.goal_achieved and attempts < agent.max_attempts:\n",
    "    state = agent.perceive()\n",
    "    action, result = agent.reason(state)\n",
    "    agent.act(action, result)\n",
    "    attempts += 1\n",
    "    print(f\"State: {state}, Action: {action}, Result: {result}, Attempts: {attempts}\")\n",
    "\n",
    "if env.goal_achieved:\n",
    "    print(f\"Goal achieved with result: {env.result}\")\n",
    "else:\n",
    "    print(\"Failed to achieve goal within the maximum number of attempts.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21990d81-c1cc-427b-9106-2a32aabc62f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Llama Response: A nice causal diagram!\n",
      "\n",
      "Let's break down the relationships:\n",
      "\n",
      "1. **Smoking → Lung Cancer**: Smoking is a direct cause of Lung Cancer.\n",
      "2. **Lung Cancer → Mortality**: Having Lung Cancer directly increases the risk of Mortality (i.e., death).\n",
      "3. **Treatment → Lung Cancer**: Treatment can affect (either positively or negatively) the occurrence or severity of Lung Cancer.\n",
      "4. **Treatment → Mortality**: Treatment has a direct effect on Mortality, which is independent of its effect on Lung Cancer.\n",
      "\n",
      "Now, considering the question: \"Does Treatment affect Mortality, considering its effect on Lung Cancer?\"\n",
      "\n",
      "The answer is: **Yes**, but with some nuance.\n",
      "\n",
      "If we ignore the direct relationship between Treatment and Mortality (relationship 4), then:\n",
      "\n",
      "* If Treatment reduces Lung Cancer (relationship 3), it would indirectly reduce Mortality (via reduced Lung Cancer → reduced Mortality).\n",
      "* Conversely, if Treatment increases Lung Cancer, it would indirectly increase Mortality (via increased Lung Cancer → increased Mortality).\n",
      "\n",
      "However, since we are also considering the direct effect of Treatment on Mortality (relationship 4), we need to acknowledge that Treatment can affect Mortality both directly and indirectly through its impact on Lung Cancer.\n",
      "\n",
      "In other words, **Treatment has a mixed or mediated effect** on Mortality: it affects Mortality not only directly but also indirectly through its influence on Lung Cancer.\n",
      "State: initial_state, Action: step_1, Result: A nice causal diagram!\n",
      "\n",
      "Let's break down the relationships:\n",
      "\n",
      "1. **Smoking → Lung Cancer**: Smoking is a direct cause of Lung Cancer.\n",
      "2. **Lung Cancer → Mortality**: Having Lung Cancer directly increases the risk of Mortality (i.e., death).\n",
      "3. **Treatment → Lung Cancer**: Treatment can affect (either positively or negatively) the occurrence or severity of Lung Cancer.\n",
      "4. **Treatment → Mortality**: Treatment has a direct effect on Mortality, which is independent of its effect on Lung Cancer.\n",
      "\n",
      "Now, considering the question: \"Does Treatment affect Mortality, considering its effect on Lung Cancer?\"\n",
      "\n",
      "The answer is: **Yes**, but with some nuance.\n",
      "\n",
      "If we ignore the direct relationship between Treatment and Mortality (relationship 4), then:\n",
      "\n",
      "* If Treatment reduces Lung Cancer (relationship 3), it would indirectly reduce Mortality (via reduced Lung Cancer → reduced Mortality).\n",
      "* Conversely, if Treatment increases Lung Cancer, it would indirectly increase Mortality (via increased Lung Cancer → increased Mortality).\n",
      "\n",
      "However, since we are also considering the direct effect of Treatment on Mortality (relationship 4), we need to acknowledge that Treatment can affect Mortality both directly and indirectly through its impact on Lung Cancer.\n",
      "\n",
      "In other words, **Treatment has a mixed or mediated effect** on Mortality: it affects Mortality not only directly but also indirectly through its influence on Lung Cancer., Attempts: 1\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Class to interact with the Llama model server\n",
    "class LlamaLLM:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "\n",
    "    def generate(self, prompt):\n",
    "        payload = {\n",
    "            \"model\": \"llama3.1\",\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False\n",
    "        }\n",
    "        headers = {\n",
    "            'Content-Type': 'application/json'\n",
    "        }\n",
    "        response = requests.post(self.url, headers=headers, data=json.dumps(payload))\n",
    "        response.raise_for_status()\n",
    "        return response.json()['response']\n",
    "\n",
    "# Environment class represents the state and goal of the environment\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.state = 'initial_state'  # Initial state\n",
    "        self.goal_achieved = False  # Goal not achieved\n",
    "        self.result = None  # Store result\n",
    "\n",
    "    def get_state(self):\n",
    "        return self.state  # Get current state\n",
    "\n",
    "    def change_state(self, result):\n",
    "        self.state = 'goal_state'  # Change to goal state\n",
    "        self.result = result  # Store result\n",
    "\n",
    "# ReActAgent class uses LlamaLLM for reasoning\n",
    "class ReActAgent:\n",
    "    def __init__(self, environment, llm, max_attempts=5):\n",
    "        self.environment = environment  # Initialize environment\n",
    "        self.llm = llm  # Llama 3.1 model\n",
    "        self.max_attempts = max_attempts  # Max attempts\n",
    "\n",
    "    def perceive(self):\n",
    "        return self.environment.get_state()  # Perceive environment state\n",
    "\n",
    "    def reason(self, state):\n",
    "        # Multi-step causal reasoning prompt based on paper example\n",
    "        if state == 'initial_state':\n",
    "            prompt = \"\"\"\n",
    "            Given the following causal relationships in a healthcare context:\n",
    "            - Smoking causes Lung Cancer\n",
    "            - Lung Cancer causes Mortality\n",
    "            - Treatment affects Lung Cancer\n",
    "            - Treatment affects Mortality directly\n",
    "            \n",
    "            Question: Does Treatment affect Mortality, considering its effect on Lung Cancer?\n",
    "            \"\"\"\n",
    "        elif state == 'step_1':\n",
    "            prompt = \"\"\"\n",
    "            Based on the causal relationships provided:\n",
    "            - Smoking causes Lung Cancer\n",
    "            - Lung Cancer causes Mortality\n",
    "            - Treatment affects Lung Cancer\n",
    "            - Treatment affects Mortality directly\n",
    "            \n",
    "            Question: What is the direct effect of Treatment on Mortality?\n",
    "            \"\"\"\n",
    "        else:\n",
    "            prompt = \"Invalid state for reasoning.\"\n",
    "\n",
    "        response = self.llm.generate(prompt).strip()\n",
    "        print(f\"Raw Llama Response: {response}\")  # Debugging: print raw response\n",
    "        if state == 'initial_state' and ('Yes' in response or 'directly' in response):\n",
    "            return 'step_1', response\n",
    "        elif state == 'step_1' and 'direct effect' in response:\n",
    "            return 'achieve_goal', response\n",
    "        return 'take_action', None\n",
    "\n",
    "    def act(self, action, result):\n",
    "        if action == 'step_1':\n",
    "            self.environment.state = 'step_1'  # Move to next step\n",
    "        elif action == 'achieve_goal':\n",
    "            self.environment.goal_achieved = True  # Achieve goal\n",
    "            self.environment.change_state(result)  # Change state and store result\n",
    "\n",
    "# Initialize the Llama model with the server URL\n",
    "llm = LlamaLLM(url=\"http://localhost:11434/api/generate\")\n",
    "\n",
    "# Create environment and agent\n",
    "env = Environment()\n",
    "agent = ReActAgent(env, llm)\n",
    "\n",
    "# Agent perception, reasoning, and action loop\n",
    "attempts = 0\n",
    "while not env.goal_achieved and attempts < agent.max_attempts:\n",
    "    state = agent.perceive()\n",
    "    action, result = agent.reason(state)\n",
    "    agent.act(action, result)\n",
    "    attempts += 1\n",
    "    print(f\"State: {state}, Action: {action}, Result: {result}, Attempts: {attempts}\")\n",
    "\n",
    "if env.goal_achieved:\n",
    "    print(f\"Goal achieved with result: {env.result}\")\n",
    "else:\n",
    "    print(\"Failed to achieve goal within the maximum number of attempts.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c86285-eebc-4369-a6b1-3908d5aa2a56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
