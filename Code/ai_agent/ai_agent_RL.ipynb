{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ac6c2964-bf5a-4bed-a3fb-75a150852bbf",
      "metadata": {
        "id": "ac6c2964-bf5a-4bed-a3fb-75a150852bbf",
        "outputId": "c60ec1d1-103c-4301-8676-b87df3960c35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model': 'llama3.2', 'created_at': '2024-12-17T13:12:20.2023776Z', 'response': 'As my knowledge cutoff is December 2023, I can tell you that as of that time, the President of the United States was Joe Biden. However, please note that this information may have changed since then.\\n\\nTo get the most up-to-date information, I recommend checking a reliable news source or the official website of the White House for the current president of the United States.', 'done': True, 'done_reason': 'stop', 'context': [128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 271, 128009, 128006, 882, 128007, 271, 14965, 374, 279, 4872, 315, 7427, 128009, 128006, 78191, 128007, 271, 2170, 856, 6677, 45379, 374, 6790, 220, 2366, 18, 11, 358, 649, 3371, 499, 430, 439, 315, 430, 892, 11, 279, 4900, 315, 279, 3723, 4273, 574, 13142, 38180, 13, 4452, 11, 4587, 5296, 430, 420, 2038, 1253, 617, 5614, 2533, 1243, 382, 1271, 636, 279, 1455, 709, 4791, 18920, 2038, 11, 358, 7079, 13598, 264, 15062, 3754, 2592, 477, 279, 4033, 3997, 315, 279, 5929, 4783, 369, 279, 1510, 4872, 315, 279, 3723, 4273, 13], 'total_duration': 1582975900, 'load_duration': 47194100, 'prompt_eval_count': 31, 'prompt_eval_duration': 610000000, 'eval_count': 77, 'eval_duration': 924000000}\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# Define the URL and the data payload\n",
        "url = \"http://localhost:11434/api/generate\"\n",
        "payload = {\n",
        "    \"model\": \"llama3.2\",\n",
        "    \"prompt\": \"who is the president of USA\",\n",
        "    \"stream\": False\n",
        "}\n",
        "\n",
        "# Set the headers\n",
        "headers = {\n",
        "    'Content-Type': 'application/json'\n",
        "}\n",
        "\n",
        "# Make the POST request\n",
        "response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
        "\n",
        "# Print the response\n",
        "print(response.json())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "557a58b7-ef33-4829-8d10-859ea00d8b31",
      "metadata": {
        "id": "557a58b7-ef33-4829-8d10-859ea00d8b31",
        "outputId": "44e14081-30f3-447d-ff9b-1a8350c7452e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw Llama Response: No, based on the provided causal relationships, Smoking does not directly affect Mortality.\n",
            "\n",
            "The relationship between Smoking and Lung Cancer is direct. Smoking causes Lung Cancer.\n",
            "\n",
            "However, the relationship between Lung Cancer and Mortality is indirect through a third variable (Lung Cancer). According to the statement \"Lung Cancer causes Mortality\", it's not clear if this is an independent effect of Lung Cancer on Mortality or if there are other intermediate variables involved that might also affect Mortality.\n",
            "\n",
            "The statement \"Smoking is independent of Mortality given Lung Cancer\" implies that while Smoking contributes to Lung Cancer, which in turn affects Mortality, the relationship between Smoking and Mortality itself is not direct. The effect of Smoking on Mortality seems to be mediated by the occurrence of Lung Cancer.\n",
            "\n",
            "Therefore, based on these relationships, it can be concluded that there is no direct causal link between Smoking and Mortality.\n",
            "State: initial_state, Action: achieve_goal, Result: Smoking does not directly affect Mortality, Attempts: 1\n",
            "Goal achieved with result: Smoking does not directly affect Mortality\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# Class to interact with the Llama model server\n",
        "class LlamaLLM:\n",
        "    def __init__(self, url):\n",
        "        self.url = url\n",
        "\n",
        "    def generate(self, prompt):\n",
        "        payload = {\n",
        "            \"model\": \"llama3.2\",\n",
        "            \"prompt\": prompt,\n",
        "            \"stream\": False\n",
        "        }\n",
        "        headers = {\n",
        "            'Content-Type': 'application/json'\n",
        "        }\n",
        "        response = requests.post(self.url, headers=headers, data=json.dumps(payload))\n",
        "        response.raise_for_status()\n",
        "        return response.json()['response']\n",
        "\n",
        "# Environment class remains the same\n",
        "class Environment:\n",
        "    def __init__(self):\n",
        "        self.state = 'initial_state'  # Initial state\n",
        "        self.goal_achieved = False  # Goal not achieved\n",
        "        self.result = None  # Store result\n",
        "\n",
        "    def get_state(self):\n",
        "        return self.state  # Get current state\n",
        "\n",
        "    def change_state(self, result):\n",
        "        self.state = 'goal_state'  # Change to goal state\n",
        "        self.result = result  # Store result\n",
        "\n",
        "# ReActAgent class modified to use LlamaLLM for reasoning\n",
        "class ReActAgent:\n",
        "    def __init__(self, environment, llm, max_attempts=5):\n",
        "        self.environment = environment  # Initialize environment\n",
        "        self.llm = llm  # Llama 3.1 model\n",
        "        self.max_attempts = max_attempts  # Max attempts\n",
        "\n",
        "    def perceive(self):\n",
        "        return self.environment.get_state()  # Perceive environment state\n",
        "\n",
        "    def reason(self, state):\n",
        "        # Causal reasoning prompt based on paper example\n",
        "        prompt = \"\"\"\n",
        "        Given the following causal relationships in a healthcare context:\n",
        "        - Smoking causes Lung Cancer\n",
        "        - Lung Cancer causes Mortality\n",
        "        - Smoking is independent of Mortality given Lung Cancer\n",
        "\n",
        "        Question: Does Smoking directly affect Mortality?\n",
        "        \"\"\"\n",
        "        response = self.llm.generate(prompt).strip()\n",
        "        print(f\"Raw Llama Response: {response}\")  # Debugging: print raw response\n",
        "        if 'No' in response or 'independent' in response:\n",
        "            return 'achieve_goal', \"Smoking does not directly affect Mortality\"\n",
        "        return 'take_action', None\n",
        "\n",
        "    def act(self, action, result):\n",
        "        if action == 'achieve_goal':\n",
        "            self.environment.goal_achieved = True  # Achieve goal\n",
        "            self.environment.change_state(result)  # Change state and store result\n",
        "\n",
        "# Initialize the Llama model with the server URL\n",
        "llm = LlamaLLM(url=\"http://localhost:11434/api/generate\")\n",
        "\n",
        "# Create environment and agent\n",
        "env = Environment()\n",
        "agent = ReActAgent(env, llm)\n",
        "\n",
        "# Agent perception, reasoning, and action loop\n",
        "attempts = 0\n",
        "while not env.goal_achieved and attempts < agent.max_attempts:\n",
        "    state = agent.perceive()\n",
        "    action, result = agent.reason(state)\n",
        "    agent.act(action, result)\n",
        "    attempts += 1\n",
        "    print(f\"State: {state}, Action: {action}, Result: {result}, Attempts: {attempts}\")\n",
        "\n",
        "if env.goal_achieved:\n",
        "    print(f\"Goal achieved with result: {env.result}\")\n",
        "else:\n",
        "    print(\"Failed to achieve goal within the maximum number of attempts.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "36a0ef5c-6944-4684-a1b4-ceb6a2ca95fd",
      "metadata": {
        "id": "36a0ef5c-6944-4684-a1b4-ceb6a2ca95fd",
        "outputId": "87d46ae2-447a-4fd3-ddb7-6c00e3fd2041",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw Llama Response: No, Smoking does not directly affect Mortality. The correct causal relationship is:\n",
            "\n",
            "- Smoking causes Lung Cancer (direct effect)\n",
            "- Lung Cancer causes Mortality (indirect effect through the development of the disease)\n",
            "\n",
            "The statement \"Smoking is independent of Mortality given Lung Cancer\" means that smoking and mortality are conditionally independent, but this does not mean that smoking has no effect on mortality. It means that while there is a relationship between smoking and lung cancer, once you have developed lung cancer, the relationship between smoking and mortality holds.\n",
            "\n",
            "In other words, even if someone stops smoking after developing lung cancer, their mortality rate will still be influenced by the presence of lung cancer.\n",
            "State: initial_state, Action: achieve_goal, Result: Smoking does not directly affect Mortality, Attempts: 1\n",
            "Goal achieved with result: Smoking does not directly affect Mortality\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# Class to interact with the Llama model server\n",
        "class LlamaLLM:\n",
        "    def __init__(self, url):\n",
        "        self.url = url\n",
        "\n",
        "    def generate(self, prompt):\n",
        "        payload = {\n",
        "            \"model\": \"llama3.2\",\n",
        "            \"prompt\": prompt,\n",
        "            \"stream\": False\n",
        "        }\n",
        "        headers = {\n",
        "            'Content-Type': 'application/json'\n",
        "        }\n",
        "        response = requests.post(self.url, headers=headers, data=json.dumps(payload))\n",
        "        response.raise_for_status()\n",
        "        return response.json()['response']\n",
        "\n",
        "# Environment class remains the same\n",
        "class Environment:\n",
        "    def __init__(self):\n",
        "        self.state = 'initial_state'  # Initial state\n",
        "        self.goal_achieved = False  # Goal not achieved\n",
        "        self.result = None  # Store result\n",
        "\n",
        "    def get_state(self):\n",
        "        return self.state  # Get current state\n",
        "\n",
        "    def change_state(self, result):\n",
        "        self.state = 'goal_state'  # Change to goal state\n",
        "        self.result = result  # Store result\n",
        "\n",
        "# ReActAgent class modified to use LlamaLLM for reasoning\n",
        "class ReActAgent:\n",
        "    def __init__(self, environment, llm, max_attempts=5):\n",
        "        self.environment = environment  # Initialize environment\n",
        "        self.llm = llm  # Llama 3.1 model\n",
        "        self.max_attempts = max_attempts  # Max attempts\n",
        "\n",
        "    def perceive(self):\n",
        "        return self.environment.get_state()  # Perceive environment state\n",
        "\n",
        "    def reason(self, state):\n",
        "        # Causal reasoning prompt based on paper example\n",
        "        prompt = \"\"\"\n",
        "        Given the following causal relationships in a healthcare context:\n",
        "        - Smoking causes Lung Cancer\n",
        "        - Lung Cancer causes Mortality\n",
        "        - Smoking is independent of Mortality given Lung Cancer\n",
        "\n",
        "        Question: Does Smoking directly affect Mortality?\n",
        "        \"\"\"\n",
        "        response = self.llm.generate(prompt).strip()\n",
        "        print(f\"Raw Llama Response: {response}\")  # Debugging: print raw response\n",
        "        if 'No' in response or 'independent' in response:\n",
        "            return 'achieve_goal', \"Smoking does not directly affect Mortality\"\n",
        "        return 'take_action', None\n",
        "\n",
        "    def act(self, action, result):\n",
        "        if action == 'achieve_goal':\n",
        "            self.environment.goal_achieved = True  # Achieve goal\n",
        "            self.environment.change_state(result)  # Change state and store result\n",
        "\n",
        "# Initialize the Llama model with the server URL\n",
        "llm = LlamaLLM(url=\"http://localhost:11434/api/generate\")\n",
        "\n",
        "# Create environment and agent\n",
        "env = Environment()\n",
        "agent = ReActAgent(env, llm)\n",
        "\n",
        "# Agent perception, reasoning, and action loop\n",
        "attempts = 0\n",
        "while not env.goal_achieved and attempts < agent.max_attempts:\n",
        "    state = agent.perceive()\n",
        "    action, result = agent.reason(state)\n",
        "    agent.act(action, result)\n",
        "    attempts += 1\n",
        "    print(f\"State: {state}, Action: {action}, Result: {result}, Attempts: {attempts}\")\n",
        "\n",
        "if env.goal_achieved:\n",
        "    print(f\"Goal achieved with result: {env.result}\")\n",
        "else:\n",
        "    print(\"Failed to achieve goal within the maximum number of attempts.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "21990d81-c1cc-427b-9106-2a32aabc62f7",
      "metadata": {
        "id": "21990d81-c1cc-427b-9106-2a32aabc62f7",
        "outputId": "781ff127-2a29-48bb-915f-8cd11b9448b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw Llama Response: Let's break down the causal relationships:\n",
            "\n",
            "1. Smoking causes Lung Cancer (IC)\n",
            "2. Lung Cancer causes Mortality (EC)\n",
            "3. Treatment affects Lung Cancer (IC) → This means that treatment can either prevent or worsen lung cancer.\n",
            "4. Treatment affects Mortality directly (EC) → This means that treatment has a direct impact on mortality.\n",
            "\n",
            "Now, let's consider the effect of Treatment on Mortality, given its effect on Lung Cancer:\n",
            "\n",
            "Since Treatment affects Lung Cancer and Lung Cancer causes Mortality, we can conclude that Treatment also affects Mortality through its impact on Lung Cancer. In other words, Treatment influences the development or progression of Lung Cancer, which in turn affects Mortality.\n",
            "\n",
            "So, yes, considering its effect on Lung Cancer, Treatment does affect Mortality.\n",
            "State: initial_state, Action: step_1, Result: Let's break down the causal relationships:\n",
            "\n",
            "1. Smoking causes Lung Cancer (IC)\n",
            "2. Lung Cancer causes Mortality (EC)\n",
            "3. Treatment affects Lung Cancer (IC) → This means that treatment can either prevent or worsen lung cancer.\n",
            "4. Treatment affects Mortality directly (EC) → This means that treatment has a direct impact on mortality.\n",
            "\n",
            "Now, let's consider the effect of Treatment on Mortality, given its effect on Lung Cancer:\n",
            "\n",
            "Since Treatment affects Lung Cancer and Lung Cancer causes Mortality, we can conclude that Treatment also affects Mortality through its impact on Lung Cancer. In other words, Treatment influences the development or progression of Lung Cancer, which in turn affects Mortality.\n",
            "\n",
            "So, yes, considering its effect on Lung Cancer, Treatment does affect Mortality., Attempts: 1\n",
            "Raw Llama Response: Based on the information provided, there is no direct causal relationship between Treatment and Mortality. The statement \"Treatment affects Lung Cancer\" implies that treatment can reduce or eliminate lung cancer, which in turn can reduce mortality.\n",
            "\n",
            "However, we cannot make a direct inference about how treatment affects mortality directly from the information provided.\n",
            "State: step_1, Action: take_action, Result: None, Attempts: 2\n",
            "Raw Llama Response: According to the given causal relationships:\n",
            "\n",
            "1. Smoking causes Lung Cancer\n",
            "2. Lung Cancer causes Mortality\n",
            "3. Treatment affects Lung Cancer (not a direct effect on Mortality)\n",
            "4. Treatment affects Mortality directly (this is what we are trying to find)\n",
            "\n",
            "Since Treatment affects Mortality directly, but only through its effect on Lung Cancer (as stated in point 3), we can conclude that:\n",
            "\n",
            "The direct effect of Treatment on Mortality is indirect and occurs through the reduction of Lung Cancer.\n",
            "\n",
            "In other words, Treatment reduces the risk of Lung Cancer, which in turn reduces the risk of Mortality.\n",
            "State: step_1, Action: achieve_goal, Result: According to the given causal relationships:\n",
            "\n",
            "1. Smoking causes Lung Cancer\n",
            "2. Lung Cancer causes Mortality\n",
            "3. Treatment affects Lung Cancer (not a direct effect on Mortality)\n",
            "4. Treatment affects Mortality directly (this is what we are trying to find)\n",
            "\n",
            "Since Treatment affects Mortality directly, but only through its effect on Lung Cancer (as stated in point 3), we can conclude that:\n",
            "\n",
            "The direct effect of Treatment on Mortality is indirect and occurs through the reduction of Lung Cancer.\n",
            "\n",
            "In other words, Treatment reduces the risk of Lung Cancer, which in turn reduces the risk of Mortality., Attempts: 3\n",
            "Goal achieved with result: According to the given causal relationships:\n",
            "\n",
            "1. Smoking causes Lung Cancer\n",
            "2. Lung Cancer causes Mortality\n",
            "3. Treatment affects Lung Cancer (not a direct effect on Mortality)\n",
            "4. Treatment affects Mortality directly (this is what we are trying to find)\n",
            "\n",
            "Since Treatment affects Mortality directly, but only through its effect on Lung Cancer (as stated in point 3), we can conclude that:\n",
            "\n",
            "The direct effect of Treatment on Mortality is indirect and occurs through the reduction of Lung Cancer.\n",
            "\n",
            "In other words, Treatment reduces the risk of Lung Cancer, which in turn reduces the risk of Mortality.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# Class to interact with the Llama model server\n",
        "class LlamaLLM:\n",
        "    def __init__(self, url):\n",
        "        self.url = url\n",
        "\n",
        "    def generate(self, prompt):\n",
        "        payload = {\n",
        "            \"model\": \"llama3.2\",\n",
        "            \"prompt\": prompt,\n",
        "            \"stream\": False\n",
        "        }\n",
        "        headers = {\n",
        "            'Content-Type': 'application/json'\n",
        "        }\n",
        "        response = requests.post(self.url, headers=headers, data=json.dumps(payload))\n",
        "        response.raise_for_status()\n",
        "        return response.json()['response']\n",
        "\n",
        "# Environment class represents the state and goal of the environment\n",
        "class Environment:\n",
        "    def __init__(self):\n",
        "        self.state = 'initial_state'  # Initial state\n",
        "        self.goal_achieved = False  # Goal not achieved\n",
        "        self.result = None  # Store result\n",
        "\n",
        "    def get_state(self):\n",
        "        return self.state  # Get current state\n",
        "\n",
        "    def change_state(self, result):\n",
        "        self.state = 'goal_state'  # Change to goal state\n",
        "        self.result = result  # Store result\n",
        "\n",
        "# ReActAgent class uses LlamaLLM for reasoning\n",
        "class ReActAgent:\n",
        "    def __init__(self, environment, llm, max_attempts=5):\n",
        "        self.environment = environment  # Initialize environment\n",
        "        self.llm = llm  # Llama 3.1 model\n",
        "        self.max_attempts = max_attempts  # Max attempts\n",
        "\n",
        "    def perceive(self):\n",
        "        return self.environment.get_state()  # Perceive environment state\n",
        "\n",
        "    def reason(self, state):\n",
        "        # Multi-step causal reasoning prompt based on paper example\n",
        "        if state == 'initial_state':\n",
        "            prompt = \"\"\"\n",
        "            Given the following causal relationships in a healthcare context:\n",
        "            - Smoking causes Lung Cancer\n",
        "            - Lung Cancer causes Mortality\n",
        "            - Treatment affects Lung Cancer\n",
        "            - Treatment affects Mortality directly\n",
        "\n",
        "            Question: Does Treatment affect Mortality, considering its effect on Lung Cancer?\n",
        "            \"\"\"\n",
        "        elif state == 'step_1':\n",
        "            prompt = \"\"\"\n",
        "            Based on the causal relationships provided:\n",
        "            - Smoking causes Lung Cancer\n",
        "            - Lung Cancer causes Mortality\n",
        "            - Treatment affects Lung Cancer\n",
        "            - Treatment affects Mortality directly\n",
        "\n",
        "            Question: What is the direct effect of Treatment on Mortality?\n",
        "            \"\"\"\n",
        "        else:\n",
        "            prompt = \"Invalid state for reasoning.\"\n",
        "\n",
        "        response = self.llm.generate(prompt).strip()\n",
        "        print(f\"Raw Llama Response: {response}\")  # Debugging: print raw response\n",
        "        if state == 'initial_state' and ('Yes' in response or 'directly' in response):\n",
        "            return 'step_1', response\n",
        "        elif state == 'step_1' and 'direct effect' in response:\n",
        "            return 'achieve_goal', response\n",
        "        return 'take_action', None\n",
        "\n",
        "    def act(self, action, result):\n",
        "        if action == 'step_1':\n",
        "            self.environment.state = 'step_1'  # Move to next step\n",
        "        elif action == 'achieve_goal':\n",
        "            self.environment.goal_achieved = True  # Achieve goal\n",
        "            self.environment.change_state(result)  # Change state and store result\n",
        "\n",
        "# Initialize the Llama model with the server URL\n",
        "llm = LlamaLLM(url=\"http://localhost:11434/api/generate\")\n",
        "\n",
        "# Create environment and agent\n",
        "env = Environment()\n",
        "agent = ReActAgent(env, llm)\n",
        "\n",
        "# Agent perception, reasoning, and action loop\n",
        "attempts = 0\n",
        "while not env.goal_achieved and attempts < agent.max_attempts:\n",
        "    state = agent.perceive()\n",
        "    action, result = agent.reason(state)\n",
        "    agent.act(action, result)\n",
        "    attempts += 1\n",
        "    print(f\"State: {state}, Action: {action}, Result: {result}, Attempts: {attempts}\")\n",
        "\n",
        "if env.goal_achieved:\n",
        "    print(f\"Goal achieved with result: {env.result}\")\n",
        "else:\n",
        "    print(\"Failed to achieve goal within the maximum number of attempts.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06c86285-eebc-4369-a6b1-3908d5aa2a56",
      "metadata": {
        "id": "06c86285-eebc-4369-a6b1-3908d5aa2a56"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}