{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac6c2964-bf5a-4bed-a3fb-75a150852bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'llama3.1', 'created_at': '2024-07-31T13:24:40.1172474Z', 'response': 'As of my last update in April 2023, the President of the United States is Joe Biden. He took office on January 20, 2021, succeeding Donald Trump. Prior to becoming President, Joe Biden served as Vice President under Barack Obama from 2009 to 2017.', 'done': True, 'done_reason': 'stop', 'context': [128006, 882, 128007, 271, 14965, 374, 279, 4872, 315, 7427, 128009, 128006, 78191, 128007, 271, 2170, 315, 856, 1566, 2713, 304, 5936, 220, 2366, 18, 11, 279, 4900, 315, 279, 3723, 4273, 374, 13142, 38180, 13, 1283, 3952, 5274, 389, 6186, 220, 508, 11, 220, 2366, 16, 11, 73820, 9641, 3420, 13, 32499, 311, 10671, 4900, 11, 13142, 38180, 10434, 439, 23270, 4900, 1234, 24448, 7250, 505, 220, 1049, 24, 311, 220, 679, 22, 13], 'total_duration': 51249649700, 'load_duration': 21295352300, 'prompt_eval_count': 16, 'prompt_eval_duration': 3972117000, 'eval_count': 61, 'eval_duration': 25964875000}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Define the URL and the data payload\n",
    "url = \"http://localhost:11434/api/generate\"\n",
    "payload = {\n",
    "    \"model\": \"llama3.1\",\n",
    "    \"prompt\": \"who is the president of USA\",\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "# Set the headers\n",
    "headers = {\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "# Make the POST request\n",
    "response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
    "\n",
    "# Print the response\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3030e8d-c55c-4b7e-9643-a4a1c4dedc0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P\n",
      "The calculation is:\n",
      "\n",
      "2 × π ≈ 2 × 3.14159 ≈ 6.28318\n",
      "\n",
      "So the result of \"2 * pi\" is approximately 6.28.\n",
      "Result: The calculation is:\n",
      "\n",
      "2 × π ≈ 2 × 3.14159 ≈ 6.28318\n",
      "\n",
      "So the result of \"2 * pi\" is approximately 6.28.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Assuming you have a function to call your local Llama server\n",
    "class LlamaLLM:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "\n",
    "    def generate(self, prompt):\n",
    "        payload = {\n",
    "            \"model\": \"llama3.1\",\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False\n",
    "        }\n",
    "        headers = {\n",
    "            'Content-Type': 'application/json'\n",
    "        }\n",
    "        response = requests.post(self.url, headers=headers, data=json.dumps(payload))\n",
    "        response.raise_for_status()\n",
    "        return response.json()['response']\n",
    "\n",
    "# Initialize your Llama model\n",
    "llm = LlamaLLM(url=\"http://localhost:11434/api/generate\")\n",
    "\n",
    "# Define the prompt template\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=\"{query}\"\n",
    ")\n",
    "\n",
    "# Create a simple function to simulate the LLMChain behavior\n",
    "def llm_chain_run(llm, prompt, query):\n",
    "    formatted_prompt = prompt.format(query=query)\n",
    "    response = llm.generate(formatted_prompt)\n",
    "    print(\"P\")\n",
    "    print(response)\n",
    "    return response\n",
    "\n",
    "# Define a function for the calculator\n",
    "def simple_calculator():\n",
    "    query = \"Calculate 2 * pi\"\n",
    "    result = llm_chain_run(llm, prompt, {\"query\": query})\n",
    "    print(f\"Result: {result}\")\n",
    "\n",
    "# Run the calculator\n",
    "simple_calculator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20ac7ad7-40cf-4bb1-b4b6-58a38642ac37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Passed: Result is approximately correct.\n",
      "Result: To calculate 2 * pi, I'll use the value of pi (π) as approximately 3.14159.\n",
      "\n",
      "2 * π = 2 * 3.14159 ≈ 6.28318\n",
      "\n",
      "So, the result is approximately 6.28.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Class to interact with the Llama model server\n",
    "class LlamaLLM:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "\n",
    "    def generate(self, prompt):\n",
    "        # Prepare the payload for the POST request\n",
    "        payload = {\n",
    "            \"model\": \"llama3.1\",\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False\n",
    "        }\n",
    "        headers = {\n",
    "            'Content-Type': 'application/json'\n",
    "        }\n",
    "        # Send the request to the Llama model server\n",
    "        response = requests.post(self.url, headers=headers, data=json.dumps(payload))\n",
    "        response.raise_for_status()\n",
    "        return response.json()['response']\n",
    "\n",
    "# Define the prompt template\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=\"{query}\"\n",
    ")\n",
    "\n",
    "# Agent class for the simple calculator\n",
    "class SimpleCalculatorAgent:\n",
    "    def __init__(self, llm, prompt):\n",
    "        self.llm = llm\n",
    "        self.prompt = prompt\n",
    "\n",
    "    def run(self, query):\n",
    "        # Format the prompt with the input query\n",
    "        formatted_prompt = self.prompt.format(query=query)\n",
    "        # Generate the response from the Llama model\n",
    "        response = self.llm.generate(formatted_prompt)\n",
    "        return self.validate_response(response)\n",
    "\n",
    "    def validate_response(self, response):\n",
    "        # Validate the response\n",
    "        try:\n",
    "            # Expected value for 2 * pi\n",
    "            expected_value = 2 * 3.14159\n",
    "            # Extract the numerical value from the response\n",
    "            if \"6.28318\" in response:\n",
    "                return \"Validation Passed: Result is approximately correct.\", response\n",
    "            else:\n",
    "                return \"Validation Failed: Result is incorrect.\", response\n",
    "        except Exception as e:\n",
    "            return f\"Error in validating the result: {e}\", response\n",
    "\n",
    "# Initialize the Llama model with the server URL\n",
    "llm = LlamaLLM(url=\"http://localhost:11434/api/generate\")\n",
    "\n",
    "# Create an instance of the SimpleCalculatorAgent\n",
    "calculator_agent = SimpleCalculatorAgent(llm, prompt)\n",
    "\n",
    "# Function to use the agent for calculating\n",
    "def use_calculator_agent():\n",
    "    query = \"Calculate 2 * pi\"\n",
    "    validation_message, result = calculator_agent.run(query)\n",
    "    print(validation_message)\n",
    "    print(f\"Result: {result}\")\n",
    "\n",
    "# Run the calculator agent\n",
    "use_calculator_agent()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f3b2e19-eee7-499d-bbfe-3014db3e5ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Llama Response: 2^5 = 32.\n",
      "State: initial_state, Action: achieve_goal, Result: 32, Attempts: 1\n",
      "Goal achieved with result: 32\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Class to interact with the Llama model server\n",
    "class LlamaLLM:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "\n",
    "    def generate(self, prompt):\n",
    "        payload = {\n",
    "            \"model\": \"llama3.1\",\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False\n",
    "        }\n",
    "        headers = {\n",
    "            'Content-Type': 'application/json'\n",
    "        }\n",
    "        response = requests.post(self.url, headers=headers, data=json.dumps(payload))\n",
    "        response.raise_for_status()\n",
    "        return response.json()['response']\n",
    "\n",
    "# Environment class remains the same\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.state = 'initial_state'  \n",
    "        self.goal_achieved = False  \n",
    "        self.result = None \n",
    "\n",
    "    def get_state(self):\n",
    "        return self.state  \n",
    "\n",
    "    def change_state(self, result):\n",
    "        self.state = 'goal_state'  \n",
    "        self.result = result  \n",
    "\n",
    "# ReActAgent class modified to use LlamaLLM for reasoning\n",
    "class ReActAgent:\n",
    "    def __init__(self, environment, llm, max_attempts=5):\n",
    "        self.environment = environment \n",
    "        self.llm = llm  # Llama 3.1 model\n",
    "        self.max_attempts = max_attempts  \n",
    "\n",
    "    def perceive(self):\n",
    "        return self.environment.get_state()  \n",
    "\n",
    "    def reason(self, state):\n",
    "        prompt = \"What is 2^5?\"\n",
    "        response = self.llm.generate(prompt).strip()\n",
    "        print(f\"Raw Llama Response: {response}\")  # Debugging: print raw response\n",
    "        # Check if the correct result is in the response\n",
    "        if '32' in response:\n",
    "            return 'achieve_goal', 32\n",
    "        return 'take_action', None\n",
    "\n",
    "    def act(self, action, result):\n",
    "        if action == 'achieve_goal':\n",
    "            self.environment.goal_achieved = True  \n",
    "            self.environment.change_state(result)  \n",
    "\n",
    "# Initialize the Llama model with the server URL\n",
    "llm = LlamaLLM(url=\"http://localhost:11434/api/generate\")\n",
    "\n",
    "# Create environment and agent\n",
    "env = Environment()\n",
    "agent = ReActAgent(env, llm)\n",
    "\n",
    "# Agent perception, reasoning, and action loop\n",
    "attempts = 0\n",
    "while not env.goal_achieved and attempts < agent.max_attempts:\n",
    "    state = agent.perceive()\n",
    "    action, result = agent.reason(state)\n",
    "    agent.act(action, result)\n",
    "    attempts += 1\n",
    "    print(f\"State: {state}, Action: {action}, Result: {result}, Attempts: {attempts}\")\n",
    "\n",
    "if env.goal_achieved:\n",
    "    print(f\"Goal achieved with result: {env.result}\")\n",
    "else:\n",
    "    print(\"Failed to achieve goal within the maximum number of attempts.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e205a953-5558-442d-9c1a-1eabb5a15c8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
