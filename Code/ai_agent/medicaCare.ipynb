{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff0ede47-85d8-46ef-8f9a-54501e9d2f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Llama Response: I can’t answer that. This information should be used in collaboration with a medical team and other relevant professionals to inform an end-of-life care plan tailored to the individual needs of the patient.\n",
      "State: initial_state, Action: take_action, Result: None, Attempts: 1\n",
      "Raw Llama Response: I can’t provide medical advice. Consider consulting a healthcare professional for guidance on making an informed decision regarding the patient’s care. Would you like to know about resources or steps for approaching this situation?\n",
      "State: initial_state, Action: take_action, Result: None, Attempts: 2\n",
      "Raw Llama Response: I can’t answer that. If you would like to discuss end-of-life care decisions with a medical professional or ethicist, I’d be happy to help.\n",
      "State: initial_state, Action: take_action, Result: None, Attempts: 3\n",
      "Raw Llama Response: I can’t answer that. This scenario presents a complex situation where the patient’s medical history, current condition, and family preferences all influence the decision-making process. It would be best to consult with palliative care specialists or ethicists in the field who could provide guidance tailored to the specifics of this case.\n",
      "State: initial_state, Action: achieve_goal, Result: palliative care, Attempts: 4\n",
      "Goal achieved with result: palliative care\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Class to interact with the Llama model server\n",
    "class LlamaLLM:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "\n",
    "    def generate(self, prompt):\n",
    "        payload = {\n",
    "            \"model\": \"llama3.1\",\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False\n",
    "        }\n",
    "        headers = {\n",
    "            'Content-Type': 'application/json'\n",
    "        }\n",
    "        response = requests.post(self.url, headers=headers, data=json.dumps(payload))\n",
    "        response.raise_for_status()\n",
    "        return response.json()['response']\n",
    "\n",
    "# Environment class for end-of-life care\n",
    "class CareEnvironment:\n",
    "    def __init__(self, patient_data):\n",
    "        self.state = 'initial_state'\n",
    "        self.goal_achieved = False\n",
    "        self.result = None\n",
    "        self.patient_data = patient_data\n",
    "\n",
    "    def get_state(self):\n",
    "        return self.state\n",
    "\n",
    "    def change_state(self, result):\n",
    "        self.state = 'goal_state'\n",
    "        self.result = result\n",
    "\n",
    "# ReActAgent class modified for end-of-life care\n",
    "class EndOfLifeCareAgent:\n",
    "    def __init__(self, environment, llm, max_attempts=5):\n",
    "        self.environment = environment\n",
    "        self.llm = llm\n",
    "        self.max_attempts = max_attempts\n",
    "\n",
    "    def perceive(self):\n",
    "        return self.environment.get_state()\n",
    "\n",
    "    def reason(self, state):\n",
    "        prompt = self.create_prompt(self.environment.patient_data)\n",
    "        response = self.llm.generate(prompt).strip()\n",
    "        print(f\"Raw Llama Response: {response}\")  # Debugging: print raw response\n",
    "        decision = self.extract_decision(response)\n",
    "        return 'achieve_goal' if decision else 'take_action', decision\n",
    "\n",
    "    def act(self, action, result):\n",
    "        if action == 'achieve_goal':\n",
    "            self.environment.goal_achieved = True\n",
    "            self.environment.change_state(result)\n",
    "\n",
    "    def create_prompt(self, patient_data):\n",
    "        prompt = f\"\"\"\n",
    "        The patient data is as follows:\n",
    "        Age: {patient_data['age']}\n",
    "        Medical History: {', '.join(patient_data['medical_history'])}\n",
    "        Current Condition: {patient_data['current_condition']}\n",
    "        Family Preferences: {', '.join(patient_data['family_preferences'])}\n",
    "        Social Media Posts: {patient_data['social_media_posts']}\n",
    "        Personal Messages: {patient_data['personal_messages']}\n",
    "        Given this information, what is the most appropriate end-of-life care decision for this patient?\n",
    "        \"\"\"\n",
    "        return prompt\n",
    "\n",
    "    def extract_decision(self, response):\n",
    "        # Extracts a decision from the Llama model's response\n",
    "        decisions = [\"palliative care\", \"comfort care\", \"aggressive treatment\", \"life support\", \"hospice\"]\n",
    "        for decision in decisions:\n",
    "            if decision in response.lower():\n",
    "                return decision\n",
    "        return None\n",
    "\n",
    "# Example patient data with more complexity\n",
    "patient_data = {\n",
    "    \"age\": 75,\n",
    "    \"medical_history\": [\"stroke\", \"diabetes\", \"hypertension\"],\n",
    "    \"current_condition\": \"unresponsive\",\n",
    "    \"family_preferences\": [\"comfort care\"],\n",
    "    \"social_media_posts\": \"I value quality of life over prolonged suffering.\",\n",
    "    \"personal_messages\": \"I don't want to be kept alive artificially if there's no hope of recovery.\"\n",
    "}\n",
    "\n",
    "# Initialize the Llama model with the server URL\n",
    "llm = LlamaLLM(url=\"http://localhost:11434/api/generate\")\n",
    "\n",
    "# Create environment and agent\n",
    "env = CareEnvironment(patient_data)\n",
    "agent = EndOfLifeCareAgent(env, llm)\n",
    "\n",
    "# Agent perception, reasoning, and action loop\n",
    "attempts = 0\n",
    "while not env.goal_achieved and attempts < agent.max_attempts:\n",
    "    state = agent.perceive()\n",
    "    action, result = agent.reason(state)\n",
    "    agent.act(action, result)\n",
    "    attempts += 1\n",
    "    print(f\"State: {state}, Action: {action}, Result: {result}, Attempts: {attempts}\")\n",
    "\n",
    "if env.goal_achieved:\n",
    "    print(f\"Goal achieved with result: {env.result}\")\n",
    "else:\n",
    "    print(\"Failed to achieve goal within the maximum number of attempts.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94a7b51-c7ab-4429-8cc2-21aef78297d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
