{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzUmFgGoB0rUxhyT7gi3DA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wanghaiy2018/AC0499_2024/blob/main/Code/ai_agent/medicaLangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import re\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Custom class to interact with the Llama model server\n",
        "class LlamaLLM:\n",
        "    def __init__(self, url: str):\n",
        "        self.url = url\n",
        "\n",
        "    def generate(self, prompt):\n",
        "        payload = {\n",
        "            \"model\": \"llama3.2\",\n",
        "            \"prompt\": str(prompt),  # Ensure the prompt is a string\n",
        "            \"stream\": False\n",
        "        }\n",
        "        headers = {\n",
        "            'Content-Type': 'application/json'\n",
        "        }\n",
        "        response = requests.post(self.url, headers=headers, data=json.dumps(payload))\n",
        "        response.raise_for_status()\n",
        "        return response.json()['response']\n",
        "\n",
        "# Define a prompt template\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"age\", \"medical_history\", \"current_condition\", \"family_preferences\", \"social_media_posts\", \"personal_messages\"],\n",
        "    template=\"\"\"\n",
        "    The patient data is as follows:\n",
        "    Age: {age}\n",
        "    Medical History: {medical_history}\n",
        "    Current Condition: {current_condition}\n",
        "    Family Preferences: {family_preferences}\n",
        "    Social Media Posts: {social_media_posts}\n",
        "    Personal Messages: {personal_messages}\n",
        "    Given this information, what is the most appropriate end-of-life care decision for this patient?\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# Environment class for end-of-life care\n",
        "class CareEnvironment:\n",
        "    def __init__(self, patient_data):\n",
        "        self.state = 'initial_state'\n",
        "        self.goal_achieved = False\n",
        "        self.result = None\n",
        "        self.patient_data = patient_data\n",
        "\n",
        "    def get_state(self):\n",
        "        return self.state\n",
        "\n",
        "    def change_state(self, result):\n",
        "        self.state = 'goal_state'\n",
        "        self.result = result\n",
        "\n",
        "# ReActAgent class modified for end-of-life care\n",
        "class EndOfLifeCareAgent:\n",
        "    def __init__(self, environment, llm, prompt_template, max_attempts=5):\n",
        "        self.environment = environment\n",
        "        self.llm = llm\n",
        "        self.prompt_template = prompt_template\n",
        "        self.max_attempts = max_attempts\n",
        "\n",
        "    def perceive(self):\n",
        "        return self.environment.get_state()\n",
        "\n",
        "    def reason(self, state):\n",
        "        prompt = self.create_prompt(self.environment.patient_data)\n",
        "        response = self.llm.generate(prompt).strip()\n",
        "        print(f\"Raw Llama Response: {response}\")  # Debugging: print raw response\n",
        "        decision = self.extract_decision(response)\n",
        "        return 'achieve_goal' if decision else 'take_action', decision\n",
        "\n",
        "    def act(self, action, result):\n",
        "        if action == 'achieve_goal':\n",
        "            self.environment.goal_achieved = True\n",
        "            self.environment.change_state(result)\n",
        "\n",
        "    def create_prompt(self, patient_data):\n",
        "        return self.prompt_template.format_prompt(\n",
        "            age=patient_data['age'],\n",
        "            medical_history=\", \".join(patient_data['medical_history']),\n",
        "            current_condition=patient_data['current_condition'],\n",
        "            family_preferences=\", \".join(patient_data['family_preferences']),\n",
        "            social_media_posts=patient_data['social_media_posts'],\n",
        "            personal_messages=patient_data['personal_messages']\n",
        "        ).to_string()\n",
        "\n",
        "    def extract_decision(self, response):\n",
        "        # Extracts a decision from the Llama model's response\n",
        "        decisions = [\"palliative care\", \"comfort care\", \"aggressive treatment\", \"life support\", \"hospice\"]\n",
        "        for decision in decisions:\n",
        "            if decision in response.lower():\n",
        "                return decision\n",
        "        return None\n",
        "\n",
        "# Example patient data\n",
        "patient_data = {\n",
        "    \"age\": 75,\n",
        "    \"medical_history\": [\"stroke\", \"diabetes\", \"hypertension\"],\n",
        "    \"current_condition\": \"unresponsive\",\n",
        "    \"family_preferences\": [\"comfort care\"],\n",
        "    \"social_media_posts\": \"I value quality of life over prolonged suffering.\",\n",
        "    \"personal_messages\": \"I don't want to be kept alive artificially if there's no hope of recovery.\"\n",
        "}\n",
        "\n",
        "# Initialize the Llama model with the server URL\n",
        "llm = LlamaLLM(url=\"http://localhost:11434/api/generate\")\n",
        "\n",
        "# Create environment and agent\n",
        "env = CareEnvironment(patient_data)\n",
        "agent = EndOfLifeCareAgent(env, llm, prompt_template)\n",
        "\n",
        "# Agent perception, reasoning, and action loop\n",
        "attempts = 0\n",
        "while not env.goal_achieved and attempts < agent.max_attempts:\n",
        "    state = agent.perceive()\n",
        "    action, result = agent.reason(state)\n",
        "    agent.act(action, result)\n",
        "    attempts += 1\n",
        "    print(f\"State: {state}, Action: {action}, Result: {result}, Attempts: {attempts}\")\n",
        "\n",
        "if env.goal_achieved:\n",
        "    print(f\"Goal achieved with result: {env.result}\")\n",
        "else:\n",
        "    print(\"Failed to achieve goal within the maximum number of attempts.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTwCNu7ysk4v",
        "outputId": "12ac9107-02a5-4ea0-85ee-b6ad90de3d08"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw Llama Response: I can't provide medical advice. If you or someone you know is in need of end-of-life care, I recommend consulting with a qualified healthcare professional, such as an oncologist, palliative care specialist, or hospice nurse, who can provide guidance based on the individual's specific needs and circumstances. Is there anything else I can help you with?\n",
            "State: initial_state, Action: achieve_goal, Result: palliative care, Attempts: 1\n",
            "Goal achieved with result: palliative care\n"
          ]
        }
      ]
    }
  ]
}