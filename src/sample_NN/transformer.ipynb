{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8228be04-f7f2-43a8-a13a-b292aa8d3256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_size, target_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.input_embed = nn.Embedding(input_size, d_model)\n",
    "        self.target_embed = nn.Embedding(target_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout, batch_first=True)\n",
    "        self.out = nn.Linear(d_model, target_size)\n",
    "\n",
    "    def forward(self, src, tgt, src_key_padding_mask=None, tgt_key_padding_mask=None):\n",
    "        src = self.input_embed(src)\n",
    "        src = self.positional_encoding(src)\n",
    "        tgt = self.target_embed(tgt)\n",
    "        tgt = self.positional_encoding(tgt)\n",
    "        output = self.transformer(src, tgt, src_key_padding_mask=src_key_padding_mask, tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "        output = self.out(output)\n",
    "        return output\n",
    "\n",
    "def train_model(model, dataloader, criterion, optimizer, num_epochs=20, pad_idx=0):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for src_batch, tgt_batch in dataloader:\n",
    "            src_batch = src_batch.to(next(model.parameters()).device)\n",
    "            tgt_batch = tgt_batch.to(next(model.parameters()).device)\n",
    "            \n",
    "            tgt_input = tgt_batch[:, :-1]\n",
    "            tgt_output = tgt_batch[:, 1:]\n",
    "            \n",
    "            src_key_padding_mask = (src_batch == pad_idx)\n",
    "            tgt_key_padding_mask = (tgt_input == pad_idx)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(src_batch, tgt_input, src_key_padding_mask=src_key_padding_mask, tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "            output = output.reshape(-1, output.shape[-1])\n",
    "            tgt_output = tgt_output.reshape(-1)\n",
    "            loss = criterion(output, tgt_output)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        average_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {average_loss:.4f}\")\n",
    "\n",
    "def generate_sequence_greedy(model, src_input, start_token_id, max_length, end_token_id, pad_idx=0):\n",
    "    model.eval()\n",
    "    src = torch.tensor([src_input], dtype=torch.long).to(next(model.parameters()).device)\n",
    "    tgt_input = [start_token_id]  # Start with the start token\n",
    "\n",
    "    for i in range(max_length):\n",
    "        tgt = torch.tensor([tgt_input], dtype=torch.long).to(next(model.parameters()).device)\n",
    "        src_key_padding_mask = (src == pad_idx)\n",
    "        tgt_key_padding_mask = (tgt == pad_idx)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(src, tgt, src_key_padding_mask=src_key_padding_mask, tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "        \n",
    "        next_token = output.argmax(-1)[:, -1].item()  # Take the most likely next token\n",
    "        tgt_input.append(next_token)\n",
    "        if next_token == end_token_id:  # Stop if the end token is generated\n",
    "            break\n",
    "\n",
    "    return tgt_input\n",
    "\n",
    "def decode_sequence(sequence, vocab):\n",
    "    return ' '.join([vocab[idx] for idx in sequence if idx not in (0, 1, 2)])  # Exclude padding, start, and end tokens\n",
    "\n",
    "# Example Dataset\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_sentences, tgt_sentences, src_vocab, tgt_vocab, max_len=10):\n",
    "        self.src_sentences = src_sentences\n",
    "        self.tgt_sentences = tgt_sentences\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src = self.src_sentences[idx]\n",
    "        tgt = self.tgt_sentences[idx]\n",
    "\n",
    "        src_ids = [self.src_vocab.get(token, self.src_vocab['<unk>']) for token in src.split()]\n",
    "        tgt_ids = [self.tgt_vocab.get(token, self.tgt_vocab['<unk>']) for token in tgt.split()]\n",
    "\n",
    "        # Pad sequences to max_len\n",
    "        src_ids = src_ids[:self.max_len] + [self.src_vocab['<pad>']] * (self.max_len - len(src_ids))\n",
    "        tgt_ids = tgt_ids[:self.max_len] + [self.tgt_vocab['<pad>']] * (self.max_len - len(tgt_ids))\n",
    "\n",
    "        return torch.tensor(src_ids, dtype=torch.long), torch.tensor(tgt_ids, dtype=torch.long)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b3616fd-015c-4665-ab28-04a268b0d27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.4094\n",
      "Epoch 2/20, Loss: 0.0034\n",
      "Epoch 3/20, Loss: 0.0016\n",
      "Epoch 4/20, Loss: 0.0012\n",
      "Epoch 5/20, Loss: 0.0010\n",
      "Epoch 6/20, Loss: 0.0009\n",
      "Epoch 7/20, Loss: 0.0008\n",
      "Epoch 8/20, Loss: 0.0007\n",
      "Epoch 9/20, Loss: 0.0006\n",
      "Epoch 10/20, Loss: 0.0005\n",
      "Epoch 11/20, Loss: 0.0005\n",
      "Epoch 12/20, Loss: 0.0004\n",
      "Epoch 13/20, Loss: 0.0004\n",
      "Epoch 14/20, Loss: 0.0004\n",
      "Epoch 15/20, Loss: 0.0003\n",
      "Epoch 16/20, Loss: 0.0003\n",
      "Epoch 17/20, Loss: 0.0003\n",
      "Epoch 18/20, Loss: 0.0003\n",
      "Epoch 19/20, Loss: 0.0003\n",
      "Epoch 20/20, Loss: 0.0002\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_size = 16  # Updated to match the vocabulary size\n",
    "target_size = 16  # Updated to match the vocabulary size\n",
    "d_model = 512\n",
    "nhead = 8\n",
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3\n",
    "dim_feedforward = 2048\n",
    "dropout = 0.1\n",
    "num_epochs = 20\n",
    "learning_rate = 0.0001\n",
    "batch_size = 32\n",
    "pad_idx = 0  # Define pad_idx before it's used\n",
    "\n",
    "# Define your vocabularies\n",
    "src_vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3, 'I': 4, 'am': 5, 'a': 6, 'student': 7, 'You': 8, 'are': 9, 'teacher': 10, 'He': 11, 'is': 12, 'doctor': 13, 'She': 14, 'nurse': 15}\n",
    "tgt_vocab = {0: '<pad>', 1: '<sos>', 2: '<eos>', 3: '<unk>', 4: 'Je', 5: 'suis', 6: 'un', 7: 'étudiant', 8: 'Vous', 9: 'êtes', 10: 'enseignant', 11: 'Il', 12: 'est', 13: 'médecin', 14: 'Elle', 15: 'infirmière'}\n",
    "\n",
    "# Invert the target vocabulary to decode sequences\n",
    "idx_to_tgt_vocab = {v: k for k, v in tgt_vocab.items()}\n",
    "\n",
    "# Example sentences\n",
    "src_sentences = [\"I am a student\", \"You are a teacher\", \"He is a doctor\", \"She is a nurse\"] * 250  # Replace with actual sentences\n",
    "tgt_sentences = [\"Je suis un étudiant\", \"Vous êtes un enseignant\", \"Il est un médecin\", \"Elle est une infirmière\"] * 250  # Replace with actual sentences\n",
    "\n",
    "\n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "dataset = TranslationDataset(src_sentences, tgt_sentences, src_vocab, idx_to_tgt_vocab)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = TransformerModel(len(src_vocab), len(idx_to_tgt_vocab), d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, dataloader, criterion, optimizer, num_epochs, pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18dd2b34-0f6d-49d5-aeff-a423d55938ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 6, 7]\n",
      "un étudiant\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "start_token_id = 1  # Start token ID, defined according to your vocabulary\n",
    "end_token_id = 2    # End token ID, defined according to your vocabulary\n",
    "max_length = 2     # Maximum length of the generated sequence\n",
    "\n",
    "# Example source input with padding\n",
    "src_input = [src_vocab[token] for token in \"I am a student\".split()] + [src_vocab['<pad>']] * (10 - len(\"I am a student\".split()))\n",
    "\n",
    "# Generate a sequence\n",
    "generated_sequence = generate_sequence_greedy(model, src_input, start_token_id, max_length, end_token_id, pad_idx)\n",
    "print(generated_sequence)\n",
    "\n",
    "# Decode the generated sequence\n",
    "decoded_sequence = decode_sequence(generated_sequence, tgt_vocab)\n",
    "print(decoded_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a2f1525-a706-4279-a99d-ecf33530b53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.4239\n",
      "Epoch 2/20, Loss: 0.0033\n",
      "Epoch 3/20, Loss: 0.0016\n",
      "Epoch 4/20, Loss: 0.0012\n",
      "Epoch 5/20, Loss: 0.0010\n",
      "Epoch 6/20, Loss: 0.0008\n",
      "Epoch 7/20, Loss: 0.0007\n",
      "Epoch 8/20, Loss: 0.0006\n",
      "Epoch 9/20, Loss: 0.0006\n",
      "Epoch 10/20, Loss: 0.0005\n",
      "Epoch 11/20, Loss: 0.0004\n",
      "Epoch 12/20, Loss: 0.0004\n",
      "Epoch 13/20, Loss: 0.0004\n",
      "Epoch 14/20, Loss: 0.0003\n",
      "Epoch 15/20, Loss: 0.0003\n",
      "Epoch 16/20, Loss: 0.0003\n",
      "Epoch 17/20, Loss: 0.0003\n",
      "Epoch 18/20, Loss: 0.0002\n",
      "Epoch 19/20, Loss: 0.0002\n",
      "Epoch 20/20, Loss: 0.0002\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_size, target_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.input_embed = nn.Embedding(input_size, d_model)\n",
    "        self.target_embed = nn.Embedding(target_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout, batch_first=True)\n",
    "        self.out = nn.Linear(d_model, target_size)\n",
    "\n",
    "    def forward(self, src, tgt, src_key_padding_mask=None, tgt_key_padding_mask=None):\n",
    "        src = self.input_embed(src)\n",
    "        src = self.positional_encoding(src)\n",
    "        tgt = self.target_embed(tgt)\n",
    "        tgt = self.positional_encoding(tgt)\n",
    "        output = self.transformer(src, tgt, src_key_padding_mask=src_key_padding_mask, tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "        output = self.out(output)\n",
    "        return output\n",
    "\n",
    "def train_model(model, dataloader, criterion, optimizer, num_epochs=20, pad_idx=0):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for src_batch, tgt_batch in dataloader:\n",
    "            src_batch = src_batch.to(next(model.parameters()).device)\n",
    "            tgt_batch = tgt_batch.to(next(model.parameters()).device)\n",
    "            \n",
    "            tgt_input = tgt_batch[:, :-1]\n",
    "            tgt_output = tgt_batch[:, 1:]\n",
    "            \n",
    "            src_key_padding_mask = (src_batch == pad_idx)\n",
    "            tgt_key_padding_mask = (tgt_input == pad_idx)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(src_batch, tgt_input, src_key_padding_mask=src_key_padding_mask, tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "            output = output.reshape(-1, output.shape[-1])\n",
    "            tgt_output = tgt_output.reshape(-1)\n",
    "            loss = criterion(output, tgt_output)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        average_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {average_loss:.4f}\")\n",
    "\n",
    "def generate_sequence_greedy(model, src_input, start_token_id, max_length, end_token_id, pad_idx=0):\n",
    "    model.eval()\n",
    "    src = torch.tensor([src_input], dtype=torch.long).to(next(model.parameters()).device)\n",
    "    tgt_input = [start_token_id]  # Start with the start token\n",
    "\n",
    "    for i in range(max_length):  # Loop for max_length steps\n",
    "        tgt = torch.tensor([tgt_input], dtype=torch.long).to(next(model.parameters()).device)\n",
    "        src_key_padding_mask = (src == pad_idx)\n",
    "        tgt_key_padding_mask = (tgt == pad_idx)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(src, tgt, src_key_padding_mask=src_key_padding_mask, tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "        \n",
    "        next_token = output.argmax(-1)[:, -1].item()  # Take the most likely next token\n",
    "        tgt_input.append(next_token)\n",
    "        if next_token == end_token_id:  # Stop if the end token is generated\n",
    "            break\n",
    "\n",
    "    return tgt_input\n",
    "\n",
    "def decode_sequence(sequence, vocab):\n",
    "    return ' '.join([vocab[idx] for idx in sequence if idx not in (0, 1, 2)])  # Exclude padding, start, and end tokens\n",
    "\n",
    "# Define your vocabularies\n",
    "src_vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3, 'I': 4, 'am': 5, 'a': 6, 'student': 7, 'You': 8, 'are': 9, 'teacher': 10, 'He': 11, 'is': 12, 'doctor': 13, 'She': 14, 'nurse': 15}\n",
    "tgt_vocab = {0: '<pad>', 1: '<sos>', 2: '<eos>', 3: '<unk>', 4: 'Je', 5: 'suis', 6: 'un', 7: 'étudiant', 8: 'Vous', 9: 'êtes', 10: 'enseignant', 11: 'Il', 12: 'est', 13: 'médecin', 14: 'Elle', 15: 'infirmière'}\n",
    "\n",
    "# Invert the target vocabulary to decode sequences\n",
    "idx_to_tgt_vocab = {v: k for k, v in tgt_vocab.items()}\n",
    "\n",
    "# Example Dataset\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_sentences, tgt_sentences, src_vocab, tgt_vocab, max_len=10):\n",
    "        self.src_sentences = src_sentences\n",
    "        self.tgt_sentences = tgt_sentences\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src = self.src_sentences[idx]\n",
    "        tgt = self.tgt_sentences[idx]\n",
    "\n",
    "        src_ids = [self.src_vocab.get(token, self.src_vocab['<unk>']) for token in src.split()]\n",
    "        tgt_ids = [self.tgt_vocab.get(token, self.tgt_vocab['<unk>']) for token in tgt.split()]\n",
    "\n",
    "        # Pad sequences to max_len\n",
    "        src_ids = src_ids[:self.max_len] + [self.src_vocab['<pad>']] * (self.max_len - len(src_ids))\n",
    "        tgt_ids = tgt_ids[:self.max_len] + [self.tgt_vocab['<pad>']] * (self.max_len - len(tgt_ids))\n",
    "\n",
    "        return torch.tensor(src_ids, dtype=torch.long), torch.tensor(tgt_ids, dtype=torch.long)\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 16  # Updated to match the vocabulary size\n",
    "target_size = 16  # Updated to match the vocabulary size\n",
    "d_model = 512\n",
    "nhead = 8\n",
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3\n",
    "dim_feedforward = 2048\n",
    "dropout = 0.1\n",
    "num_epochs = 20\n",
    "learning_rate = 0.0001\n",
    "batch_size = 32\n",
    "pad_idx = 0  # Define pad_idx before it's used\n",
    "\n",
    "# Example sentences\n",
    "src_sentences = [\"I am a student\", \"You are a teacher\", \"He is a doctor\", \"She is a nurse\"] * 250  # Replace with actual sentences\n",
    "tgt_sentences = [\"Je suis un étudiant\", \"Vous êtes un enseignant\", \"Il est un médecin\", \"Elle est une infirmière\"] * 250  # Replace with actual sentences\n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "dataset = TranslationDataset(src_sentences, tgt_sentences, src_vocab, idx_to_tgt_vocab)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = TransformerModel(len(src_vocab), len(idx_to_tgt_vocab), d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, dataloader, criterion, optimizer, num_epochs, pad_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d21e217-481c-4476-9aec-ba233d3236ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_input: [4, 5, 6, 7, 0, 0, 0, 0, 0, 0]\n",
      "generated_sequence: [1, 6, 7, 6]\n",
      "decoded_sequence: un étudiant un\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "start_token_id = 1  # Start token ID, defined according to your vocabulary\n",
    "end_token_id = 2    # End token ID, defined according to your vocabulary\n",
    "max_length = 3     # Maximum length of the generated sequence\n",
    "\n",
    "# Example source input with padding\n",
    "src_input = [src_vocab[token] for token in \"I am a student\".split()] + [src_vocab['<pad>']] * (10 - len(\"I am a student\".split()))\n",
    "print(f\"src_input: {src_input}\")\n",
    "\n",
    "# Generate a sequence\n",
    "generated_sequence = generate_sequence_greedy(model, src_input, start_token_id, max_length, end_token_id, pad_idx)\n",
    "print(f\"generated_sequence: {generated_sequence}\")\n",
    "\n",
    "# Decode the generated sequence\n",
    "decoded_sequence = decode_sequence(generated_sequence, tgt_vocab)\n",
    "print(f\"decoded_sequence: {decoded_sequence}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d89c7fa6-7005-41a0-96d9-fd4e968259e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.3709\n",
      "Epoch 2/20, Loss: 0.0029\n",
      "Epoch 3/20, Loss: 0.0014\n",
      "Epoch 4/20, Loss: 0.0011\n",
      "Epoch 5/20, Loss: 0.0009\n",
      "Epoch 6/20, Loss: 0.0008\n",
      "Epoch 7/20, Loss: 0.0007\n",
      "Epoch 8/20, Loss: 0.0006\n",
      "Epoch 9/20, Loss: 0.0006\n",
      "Epoch 10/20, Loss: 0.0005\n",
      "Epoch 11/20, Loss: 0.0004\n",
      "Epoch 12/20, Loss: 0.0004\n",
      "Epoch 13/20, Loss: 0.0004\n",
      "Epoch 14/20, Loss: 0.0003\n",
      "Epoch 15/20, Loss: 0.0003\n",
      "Epoch 16/20, Loss: 0.0003\n",
      "Epoch 17/20, Loss: 0.0003\n",
      "Epoch 18/20, Loss: 0.0003\n",
      "Epoch 19/20, Loss: 0.0002\n",
      "Epoch 20/20, Loss: 0.0002\n",
      "src_input: [4, 5, 6, 7, 0, 0, 0, 0, 0, 0]\n",
      "generated_sequence: [1, 5, 6, 7, 6, 7, 6, 7, 6, 7, 6]\n",
      "decoded_sequence: suis un étudiant un étudiant un étudiant un étudiant un\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_size, target_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.input_embed = nn.Embedding(input_size, d_model)\n",
    "        self.target_embed = nn.Embedding(target_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout, batch_first=True)\n",
    "        self.out = nn.Linear(d_model, target_size)\n",
    "\n",
    "    def forward(self, src, tgt, src_key_padding_mask=None, tgt_key_padding_mask=None):\n",
    "        src = self.input_embed(src)\n",
    "        src = self.positional_encoding(src)\n",
    "        tgt = self.target_embed(tgt)\n",
    "        tgt = self.positional_encoding(tgt)\n",
    "        output = self.transformer(src, tgt, src_key_padding_mask=src_key_padding_mask, tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "        output = self.out(output)\n",
    "        return output\n",
    "\n",
    "def train_model(model, dataloader, criterion, optimizer, num_epochs=20, pad_idx=0):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for src_batch, tgt_batch in dataloader:\n",
    "            src_batch = src_batch.to(next(model.parameters()).device)\n",
    "            tgt_batch = tgt_batch.to(next(model.parameters()).device)\n",
    "            \n",
    "            tgt_input = tgt_batch[:, :-1]\n",
    "            tgt_output = tgt_batch[:, 1:]\n",
    "            \n",
    "            src_key_padding_mask = (src_batch == pad_idx)\n",
    "            tgt_key_padding_mask = (tgt_input == pad_idx)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(src_batch, tgt_input, src_key_padding_mask=src_key_padding_mask, tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "            output = output.reshape(-1, output.shape[-1])\n",
    "            tgt_output = tgt_output.reshape(-1)\n",
    "            loss = criterion(output, tgt_output)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        average_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {average_loss:.4f}\")\n",
    "\n",
    "def generate_sequence_greedy(model, src_input, start_token_id, max_length, end_token_id, pad_idx=0):\n",
    "    model.eval()\n",
    "    src = torch.tensor([src_input], dtype=torch.long).to(next(model.parameters()).device)\n",
    "    tgt_input = [start_token_id]  # Start with the start token\n",
    "\n",
    "    for i in range(max_length):  # Loop for max_length steps\n",
    "        tgt = torch.tensor([tgt_input], dtype=torch.long).to(next(model.parameters()).device)\n",
    "        src_key_padding_mask = (src == pad_idx)\n",
    "        tgt_key_padding_mask = (tgt == pad_idx)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(src, tgt, src_key_padding_mask=src_key_padding_mask, tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "        \n",
    "        next_token = output.argmax(-1)[:, -1].item()  # Take the most likely next token\n",
    "        tgt_input.append(next_token)\n",
    "        if next_token == end_token_id:  # Stop if the end token is generated\n",
    "            break\n",
    "\n",
    "    return tgt_input\n",
    "\n",
    "def decode_sequence(sequence, vocab):\n",
    "    return ' '.join([vocab[idx] for idx in sequence if idx not in (0, 1, 2)])  # Exclude padding, start, and end tokens\n",
    "\n",
    "# Define your vocabularies\n",
    "src_vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3, 'I': 4, 'am': 5, 'a': 6, 'student': 7, 'You': 8, 'are': 9, 'teacher': 10, 'He': 11, 'is': 12, 'doctor': 13, 'She': 14, 'nurse': 15}\n",
    "tgt_vocab = {0: '<pad>', 1: '<sos>', 2: '<eos>', 3: '<unk>', 4: 'Je', 5: 'suis', 6: 'un', 7: 'étudiant', 8: 'Vous', 9: 'êtes', 10: 'enseignant', 11: 'Il', 12: 'est', 13: 'médecin', 14: 'Elle', 15: 'infirmière'}\n",
    "\n",
    "# Invert the target vocabulary to decode sequences\n",
    "idx_to_tgt_vocab = {v: k for k, v in tgt_vocab.items()}\n",
    "\n",
    "# Example Dataset\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_sentences, tgt_sentences, src_vocab, tgt_vocab, max_len=10):\n",
    "        self.src_sentences = src_sentences\n",
    "        self.tgt_sentences = tgt_sentences\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src = self.src_sentences[idx]\n",
    "        tgt = self.tgt_sentences[idx]\n",
    "\n",
    "        src_ids = [self.src_vocab.get(token, self.src_vocab['<unk>']) for token in src.split()]\n",
    "        tgt_ids = [self.tgt_vocab.get(token, self.tgt_vocab['<unk>']) for token in tgt.split()]\n",
    "\n",
    "        # Pad sequences to max_len\n",
    "        src_ids = src_ids[:self.max_len] + [self.src_vocab['<pad>']] * (self.max_len - len(src_ids))\n",
    "        tgt_ids = tgt_ids[:self.max_len] + [self.tgt_vocab['<pad>']] * (self.max_len - len(tgt_ids))\n",
    "\n",
    "        return torch.tensor(src_ids, dtype=torch.long), torch.tensor(tgt_ids, dtype=torch.long)\n",
    "\n",
    "# Example usage\n",
    "start_token_id = 1  # Start token ID, defined according to your vocabulary\n",
    "end_token_id = 2    # End token ID, defined according to your vocabulary\n",
    "max_length = 10     # Maximum length of the generated sequence\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 16  # Updated to match the vocabulary size\n",
    "target_size = 16  # Updated to match the vocabulary size\n",
    "d_model = 512\n",
    "nhead = 8\n",
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3\n",
    "dim_feedforward = 2048\n",
    "dropout = 0.1\n",
    "num_epochs = 20\n",
    "learning_rate = 0.0001\n",
    "batch_size = 32\n",
    "pad_idx = 0  # Define pad_idx before it's used\n",
    "\n",
    "# Example sentences\n",
    "src_sentences = [\"I am a student\", \"You are a teacher\", \"He is a doctor\", \"She is a nurse\"] * 250  # Replace with actual sentences\n",
    "tgt_sentences = [\"Je suis un étudiant\", \"Vous êtes un enseignant\", \"Il est un médecin\", \"Elle est une infirmière\"] * 250  # Replace with actual sentences\n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "dataset = TranslationDataset(src_sentences, tgt_sentences, src_vocab, idx_to_tgt_vocab)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = TransformerModel(len(src_vocab), len(idx_to_tgt_vocab), d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, dataloader, criterion, optimizer, num_epochs, pad_idx)\n",
    "\n",
    "# Example source input with padding\n",
    "src_input = [src_vocab[token] for token in \"I am a student\".split()] + [src_vocab['<pad>']] * (10 - len(\"I am a student\".split()))\n",
    "print(f\"src_input: {src_input}\")\n",
    "\n",
    "# Generate a sequence\n",
    "generated_sequence = generate_sequence_greedy(model, src_input, start_token_id, max_length, end_token_id, pad_idx)\n",
    "print(f\"generated_sequence: {generated_sequence}\")\n",
    "\n",
    "# Decode the generated sequence\n",
    "decoded_sequence = decode_sequence(generated_sequence, tgt_vocab)\n",
    "print(f\"decoded_sequence: {decoded_sequence}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bbdfdc3-b310-40d1-bfac-388baad3584c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_input: [4, 5, 6, 7, 0, 0, 0, 0, 0, 0]\n",
      "generated_sequence: [1, 5, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6]\n",
      "decoded_sequence: suis un étudiant un étudiant un étudiant un étudiant un étudiant un\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "start_token_id = 1  # Start token ID, defined according to your vocabulary\n",
    "end_token_id = 2    # End token ID, defined according to your vocabulary\n",
    "max_length = 12     # Maximum length of the generated sequence\n",
    "\n",
    "\n",
    "# Example source input with padding\n",
    "src_input = [src_vocab[token] for token in \"I am a student\".split()] + [src_vocab['<pad>']] * (10 - len(\"I am a student\".split()))\n",
    "print(f\"src_input: {src_input}\")\n",
    "\n",
    "# Generate a sequence\n",
    "generated_sequence = generate_sequence_greedy(model, src_input, start_token_id, max_length, end_token_id, pad_idx)\n",
    "print(f\"generated_sequence: {generated_sequence}\")\n",
    "\n",
    "# Decode the generated sequence\n",
    "decoded_sequence = decode_sequence(generated_sequence, tgt_vocab)\n",
    "print(f\"decoded_sequence: {decoded_sequence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90858d3a-b9f4-4eba-87ab-57a8c384af23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.4620\n",
      "Epoch 2/20, Loss: 0.0032\n",
      "Epoch 3/20, Loss: 0.0014\n",
      "Epoch 4/20, Loss: 0.0011\n",
      "Epoch 5/20, Loss: 0.0009\n",
      "Epoch 6/20, Loss: 0.0008\n",
      "Epoch 7/20, Loss: 0.0007\n",
      "Epoch 8/20, Loss: 0.0006\n",
      "Epoch 9/20, Loss: 0.0005\n",
      "Epoch 10/20, Loss: 0.0005\n",
      "Epoch 11/20, Loss: 0.0004\n",
      "Epoch 12/20, Loss: 0.0004\n",
      "Epoch 13/20, Loss: 0.0004\n",
      "Epoch 14/20, Loss: 0.0003\n",
      "Epoch 15/20, Loss: 0.0003\n",
      "Epoch 16/20, Loss: 0.0003\n",
      "Epoch 17/20, Loss: 0.0003\n",
      "Epoch 18/20, Loss: 0.0003\n",
      "Epoch 19/20, Loss: 0.0002\n",
      "Epoch 20/20, Loss: 0.0002\n",
      "src_input: [4, 5, 6, 7, 0, 0, 0, 0, 0, 0]\n",
      "generated_sequence: [1, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7]\n",
      "decoded_sequence: un étudiant un étudiant un étudiant un étudiant un étudiant\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_size, target_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.input_embed = nn.Embedding(input_size, d_model)\n",
    "        self.target_embed = nn.Embedding(target_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout, batch_first=True)\n",
    "        self.out = nn.Linear(d_model, target_size)\n",
    "\n",
    "    def forward(self, src, tgt, src_key_padding_mask=None, tgt_key_padding_mask=None):\n",
    "        src = self.input_embed(src)\n",
    "        src = self.positional_encoding(src)\n",
    "        tgt = self.target_embed(tgt)\n",
    "        tgt = self.positional_encoding(tgt)\n",
    "        output = self.transformer(src, tgt, src_key_padding_mask=src_key_padding_mask, tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "        output = self.out(output)\n",
    "        return output\n",
    "\n",
    "def train_model(model, dataloader, criterion, optimizer, num_epochs=20, pad_idx=0):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for src_batch, tgt_batch in dataloader:\n",
    "            src_batch = src_batch.to(next(model.parameters()).device)\n",
    "            tgt_batch = tgt_batch.to(next(model.parameters()).device)\n",
    "            \n",
    "            tgt_input = tgt_batch[:, :-1]\n",
    "            tgt_output = tgt_batch[:, 1:]\n",
    "            \n",
    "            src_key_padding_mask = (src_batch == pad_idx)\n",
    "            tgt_key_padding_mask = (tgt_input == pad_idx)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(src_batch, tgt_input, src_key_padding_mask=src_key_padding_mask, tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "            output = output.reshape(-1, output.shape[-1])\n",
    "            tgt_output = tgt_output.reshape(-1)\n",
    "            loss = criterion(output, tgt_output)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        average_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {average_loss:.4f}\")\n",
    "\n",
    "def generate_sequence_greedy(model, src_input, start_token_id, max_length, end_token_id, pad_idx=0):\n",
    "    model.eval()\n",
    "    src = torch.tensor([src_input], dtype=torch.long).to(next(model.parameters()).device)\n",
    "    tgt_input = [start_token_id]  # Start with the start token\n",
    "\n",
    "    for i in range(max_length):  # Loop for max_length steps\n",
    "        tgt = torch.tensor([tgt_input], dtype=torch.long).to(next(model.parameters()).device)\n",
    "        src_key_padding_mask = (src == pad_idx)\n",
    "        tgt_key_padding_mask = (tgt == pad_idx)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(src, tgt, src_key_padding_mask=src_key_padding_mask, tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "        \n",
    "        next_token = output.argmax(-1)[:, -1].item()  # Take the most likely next token\n",
    "        tgt_input.append(next_token)\n",
    "        if next_token == end_token_id:  # Stop if the end token is generated\n",
    "            break\n",
    "\n",
    "    return tgt_input\n",
    "\n",
    "def decode_sequence(sequence, vocab):\n",
    "    return ' '.join([vocab[idx] for idx in sequence if idx not in (0, 1, 2)])  # Exclude padding, start, and end tokens\n",
    "\n",
    "# Define your vocabularies\n",
    "src_vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3, 'I': 4, 'am': 5, 'a': 6, 'student': 7, 'You': 8, 'are': 9, 'teacher': 10, 'He': 11, 'is': 12, 'doctor': 13, 'She': 14, 'nurse': 15}\n",
    "tgt_vocab = {0: '<pad>', 1: '<sos>', 2: '<eos>', 3: '<unk>', 4: 'Je', 5: 'suis', 6: 'un', 7: 'étudiant', 8: 'Vous', 9: 'êtes', 10: 'enseignant', 11: 'Il', 12: 'est', 13: 'médecin', 14: 'Elle', 15: 'infirmière'}\n",
    "\n",
    "# Invert the target vocabulary to decode sequences\n",
    "idx_to_tgt_vocab = {v: k for k, v in tgt_vocab.items()}\n",
    "\n",
    "# Example Dataset\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_sentences, tgt_sentences, src_vocab, tgt_vocab, max_len=10):\n",
    "        self.src_sentences = src_sentences\n",
    "        self.tgt_sentences = tgt_sentences\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src = self.src_sentences[idx]\n",
    "        tgt = self.tgt_sentences[idx]\n",
    "\n",
    "        src_ids = [self.src_vocab.get(token, self.src_vocab['<unk>']) for token in src.split()]\n",
    "        tgt_ids = [self.tgt_vocab.get(token, self.tgt_vocab['<unk>']) for token in tgt.split()]\n",
    "\n",
    "        # Pad sequences to max_len\n",
    "        src_ids = src_ids[:self.max_len] + [self.src_vocab['<pad>']] * (self.max_len - len(src_ids))\n",
    "        tgt_ids = tgt_ids[:self.max_len] + [self.tgt_vocab['<pad>']] * (self.max_len - len(tgt_ids))\n",
    "\n",
    "        return torch.tensor(src_ids, dtype=torch.long), torch.tensor(tgt_ids, dtype=torch.long)\n",
    "\n",
    "# Example usage\n",
    "start_token_id = 1  # Start token ID, defined according to your vocabulary\n",
    "end_token_id = 2    # End token ID, defined according to your vocabulary\n",
    "max_length = 10     # Maximum length of the generated sequence\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 16  # Updated to match the vocabulary size\n",
    "target_size = 16  # Updated to match the vocabulary size\n",
    "d_model = 512\n",
    "nhead = 8\n",
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3\n",
    "dim_feedforward = 2048\n",
    "dropout = 0.1\n",
    "num_epochs = 20\n",
    "learning_rate = 0.0001\n",
    "batch_size = 32\n",
    "pad_idx = 0  # Define pad_idx before it's used\n",
    "\n",
    "# Example sentences\n",
    "src_sentences = [\"I am a student\", \"You are a teacher\", \"He is a doctor\", \"She is a nurse\"] * 250  # Replace with actual sentences\n",
    "tgt_sentences = [\"Je suis un étudiant\", \"Vous êtes un enseignant\", \"Il est un médecin\", \"Elle est une infirmière\"] * 250  # Replace with actual sentences\n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "dataset = TranslationDataset(src_sentences, tgt_sentences, src_vocab, idx_to_tgt_vocab)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = TransformerModel(len(src_vocab), len(idx_to_tgt_vocab), d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, dataloader, criterion, optimizer, num_epochs, pad_idx)\n",
    "\n",
    "# Example source input with padding\n",
    "src_input = [src_vocab[token] for token in \"I am a student\".split()] + [src_vocab['<pad>']] * (10 - len(\"I am a student\".split()))\n",
    "print(f\"src_input: {src_input}\")\n",
    "\n",
    "# Generate a sequence\n",
    "generated_sequence = generate_sequence_greedy(model, src_input, start_token_id, max_length, end_token_id, pad_idx)\n",
    "print(f\"generated_sequence: {generated_sequence}\")\n",
    "\n",
    "# Decode the generated sequence\n",
    "decoded_sequence = decode_sequence(generated_sequence, tgt_vocab)\n",
    "print(f\"decoded_sequence: {decoded_sequence}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adad92b1-64ce-4510-a060-dbc9b48b875e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qnn",
   "language": "python",
   "name": "qnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
