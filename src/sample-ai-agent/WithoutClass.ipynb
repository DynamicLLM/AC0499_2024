{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DynamicLLM/LLM2024/blob/main/src/sample-ai-agent/WithoutClass.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFUY1WhlKxkS",
        "outputId": "e4c149c0-776c-4824-bae7-5b1f878f4185"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw Llama Response: I can't provide medical advice. If you or someone you know is in need of end-of-life care, I recommend consulting a qualified healthcare professional, such as an oncologist, palliative care specialist, or primary care physician. They can help facilitate open and honest discussions about end-of-life care options, including hospice care, palliative care, and other support services. Is there anything else I can help you with?\n",
            "State: initial_state, Action: achieve_goal, Result: palliative care, Attempts: 1\n",
            "Goal achieved with result: palliative care\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# Function to interact with the Llama model server\n",
        "def generate_response(url, prompt):\n",
        "    payload = {\n",
        "        \"model\": \"llama3.2\",\n",
        "        \"prompt\": str(prompt),\n",
        "        \"stream\": False\n",
        "    }\n",
        "    headers = {\n",
        "        'Content-Type': 'application/json'\n",
        "    }\n",
        "    response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
        "    response.raise_for_status()\n",
        "    return response.json()['response']\n",
        "\n",
        "# Function to create the prompt\n",
        "def create_prompt(patient_data):\n",
        "    return f\"\"\"\n",
        "    The patient data is as follows:\n",
        "    Age: {patient_data['age']}\n",
        "    Medical History: {', '.join(patient_data['medical_history'])}\n",
        "    Current Condition: {patient_data['current_condition']}\n",
        "    Family Preferences: {', '.join(patient_data['family_preferences'])}\n",
        "    Social Media Posts: {patient_data['social_media_posts']}\n",
        "    Personal Messages: {patient_data['personal_messages']}\n",
        "    Given this information, what is the most appropriate end-of-life care decision for this patient?\n",
        "    \"\"\"\n",
        "\n",
        "# Function to extract the decision from the Llama model's response\n",
        "def extract_decision(response):\n",
        "    decisions = [\"palliative care\", \"comfort care\", \"aggressive treatment\", \"life support\", \"hospice\"]\n",
        "    for decision in decisions:\n",
        "        if decision in response.lower():\n",
        "            return decision\n",
        "    return None\n",
        "\n",
        "# Function to simulate the environment and decision-making process\n",
        "def simulate_environment(url, patient_data, max_attempts=5):\n",
        "    state = 'initial_state'\n",
        "    goal_achieved = False\n",
        "    result = None\n",
        "    attempts = 0\n",
        "\n",
        "    while not goal_achieved and attempts < max_attempts:\n",
        "        # Perceive the state (simplified as a static state in this case)\n",
        "        current_state = state\n",
        "\n",
        "        # Reason based on the current state and patient data\n",
        "        prompt = create_prompt(patient_data)\n",
        "        response = generate_response(url, prompt).strip()\n",
        "        print(f\"Raw Llama Response: {response}\")  # Debugging: print raw response\n",
        "        decision = extract_decision(response)\n",
        "\n",
        "        if decision:\n",
        "            action = 'achieve_goal'\n",
        "            result = decision\n",
        "            goal_achieved = True\n",
        "        else:\n",
        "            action = 'take_action'\n",
        "\n",
        "        # Act based on the reasoning result\n",
        "        if action == 'achieve_goal':\n",
        "            state = 'goal_state'\n",
        "\n",
        "        attempts += 1\n",
        "        print(f\"State: {current_state}, Action: {action}, Result: {result}, Attempts: {attempts}\")\n",
        "\n",
        "    if goal_achieved:\n",
        "        print(f\"Goal achieved with result: {result}\")\n",
        "    else:\n",
        "        print(\"Failed to achieve goal within the maximum number of attempts.\")\n",
        "\n",
        "# Example patient data\n",
        "patient_data = {\n",
        "    \"age\": 75,\n",
        "    \"medical_history\": [\"stroke\", \"diabetes\", \"hypertension\"],\n",
        "    \"current_condition\": \"unresponsive\",\n",
        "    \"family_preferences\": [\"comfort care\"],\n",
        "    \"social_media_posts\": \"I value quality of life over prolonged suffering.\",\n",
        "    \"personal_messages\": \"I don't want to be kept alive artificially if there's no hope of recovery.\"\n",
        "}\n",
        "\n",
        "# URL of the Llama model server\n",
        "llm_url = \"http://localhost:11434/api/generate\"\n",
        "\n",
        "# Simulate the environment and decision-making process\n",
        "simulate_environment(llm_url, patient_data)\n"
      ]
    }
  ]
}