{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtsbkSxXJaVtIpxVNUwLtY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DynamicLLM/LLM2024/blob/main/src/sample-ai-agent/RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This program implements a Retrieval-Augmented Generation (RAG) system that combines semantic search and dynamic text generation. It uses SQLite to cache user queries and their corresponding responses, leveraging sentence embeddings from SentenceTransformer to find semantically similar questions based on cosine similarity. If a similar question is found, the cached response is returned. If no match is found, the program generates a new response dynamically using OpenAI's GPT model (gpt-4o-mini). The generated response is then stored in the SQLite database for future use. This hybrid approach optimizes response accuracy while reducing redundant computations, making it ideal for chatbots, FAQ systems, and knowledge-base applications."
      ],
      "metadata": {
        "id": "Y2ijMlf8-lq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from scipy.spatial.distance import cosine\n",
        "import sqlite3\n",
        "import json\n",
        "from openai import OpenAI\n",
        "\n",
        "# Initialize the embedding model\n",
        "embedding_model = SentenceTransformer('distiluse-base-multilingual-cased-v1')\n",
        "\n",
        "# Cache settings\n",
        "CACHE_EXPIRATION = 3600  # 1 hour in seconds\n",
        "SIMILARITY_THRESHOLD = 0.8  # Threshold for similarity\n",
        "DB_NAME = \"cache.db\"\n",
        "\n",
        "# Initialize OpenAI client\n",
        "client = OpenAI(\n",
        "    api_key=\"sk-proj-****A\"\n",
        ")\n",
        "\n",
        "# Initialize SQLite database\n",
        "conn = sqlite3.connect(DB_NAME)\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Create table if not exists\n",
        "cursor.execute('''\n",
        "CREATE TABLE IF NOT EXISTS cache (\n",
        "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "    question TEXT UNIQUE,\n",
        "    response TEXT,\n",
        "    embedding TEXT,\n",
        "    timestamp REAL\n",
        ")\n",
        "''')\n",
        "conn.commit()\n",
        "\n",
        "\n",
        "def set_cached_response(question, response, embedding):\n",
        "    \"\"\"Stores the question, response, and embedding in SQLite.\"\"\"\n",
        "    try:\n",
        "        timestamp = time.time()\n",
        "        embedding_str = json.dumps(embedding.tolist())  # Convert embedding to JSON string\n",
        "        cursor.execute('''\n",
        "        INSERT OR REPLACE INTO cache (question, response, embedding, timestamp)\n",
        "        VALUES (?, ?, ?, ?)\n",
        "        ''', (question, response, embedding_str, timestamp))\n",
        "        conn.commit()\n",
        "        print(f\"Cached response for: {question}\")\n",
        "    except sqlite3.Error as e:\n",
        "        print(f\"Failed to set cache: {str(e)}\")\n",
        "\n",
        "\n",
        "def find_similar_question(question, embedding):\n",
        "    \"\"\"Searches for a semantically similar question in the SQLite database.\"\"\"\n",
        "    try:\n",
        "        current_time = time.time()\n",
        "        cursor.execute('SELECT question, response, embedding, timestamp FROM cache')\n",
        "        rows = cursor.fetchall()\n",
        "\n",
        "        for row in rows:\n",
        "            cached_question, cached_response, cached_embedding_str, timestamp = row\n",
        "            if current_time - timestamp < CACHE_EXPIRATION:\n",
        "                cached_embedding = np.array(json.loads(cached_embedding_str))\n",
        "                similarity = 1 - cosine(embedding, cached_embedding)\n",
        "                if similarity > SIMILARITY_THRESHOLD:\n",
        "                    print(f\"Found similar question with similarity: {similarity}\")\n",
        "                    return cached_question, cached_response\n",
        "    except sqlite3.Error as e:\n",
        "        print(f\"Failed to search cache: {str(e)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error during cache search: {str(e)}\")\n",
        "    return None, None\n",
        "\n",
        "\n",
        "def generate_response(query):\n",
        "    \"\"\"Generates a response using OpenAI's GPT model.\"\"\"\n",
        "    try:\n",
        "        completion = client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            store=True,\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": query}\n",
        "            ]\n",
        "        )\n",
        "        # Correctly access the 'content' property\n",
        "        return completion.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to generate response: {str(e)}\")\n",
        "        return \"I'm sorry, I couldn't generate a response at this time.\"\n",
        "\n",
        "\n",
        "def get_embedding(text):\n",
        "    \"\"\"Generates an embedding for the input text.\"\"\"\n",
        "    return embedding_model.encode([text])[0]\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    user_question = input(\"Enter your question: \")\n",
        "    user_embedding = get_embedding(user_question)\n",
        "\n",
        "    # Search for similar questions in the cache\n",
        "    cached_question, cached_response = find_similar_question(user_question, user_embedding)\n",
        "\n",
        "    if cached_response:\n",
        "        print(f\"Found similar question: {cached_question}\")\n",
        "        print(f\"Cached response: {cached_response}\")\n",
        "    else:\n",
        "        print(\"No similar question found. Generating a new response...\")\n",
        "        generated_response = generate_response(user_question)\n",
        "        print(f\"Generated response: {generated_response}\")\n",
        "\n",
        "        # Cache the generated response\n",
        "        set_cached_response(user_question, generated_response, user_embedding)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sGoP-Wn9tIz",
        "outputId": "dc463140-634e-4dc2-ce1f-93d197a203a0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your question: who is you\n",
            "Found similar question with similarity: 0.9860526956352109\n",
            "Found similar question: who are you\n",
            "Cached response: I am an AI language model designed to assist with information, answer questions, and engage in conversation on a wide range of topics. How can I help you today?\n"
          ]
        }
      ]
    }
  ]
}