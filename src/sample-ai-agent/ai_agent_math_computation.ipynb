{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DynamicLLM/LLM2024/blob/main/src/sample-ai-agent/ai_agent_math_computation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac6c2964-bf5a-4bed-a3fb-75a150852bbf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac6c2964-bf5a-4bed-a3fb-75a150852bbf",
        "outputId": "8b5bbc8b-15fb-4ccf-ae3b-397d8f8c82c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model': 'llama3.2', 'created_at': '2024-12-17T13:24:53.5283074Z', 'response': 'As of my knowledge cutoff in December 2023, Joe Biden is the President of the United States. However, please note that this information may have changed since my knowledge cutoff date. For the most up-to-date information, I recommend checking with a reliable news source or official government websites.', 'done': True, 'done_reason': 'stop', 'context': [128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 271, 128009, 128006, 882, 128007, 271, 14965, 374, 279, 4872, 315, 7427, 128009, 128006, 78191, 128007, 271, 2170, 315, 856, 6677, 45379, 304, 6790, 220, 2366, 18, 11, 13142, 38180, 374, 279, 4900, 315, 279, 3723, 4273, 13, 4452, 11, 4587, 5296, 430, 420, 2038, 1253, 617, 5614, 2533, 856, 6677, 45379, 2457, 13, 1789, 279, 1455, 709, 4791, 18920, 2038, 11, 358, 7079, 13598, 449, 264, 15062, 3754, 2592, 477, 4033, 3109, 13335, 13], 'total_duration': 1454561000, 'load_duration': 41095500, 'prompt_eval_count': 31, 'prompt_eval_duration': 657000000, 'eval_count': 59, 'eval_duration': 754000000}\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# Define the URL and the data payload\n",
        "url = \"http://localhost:11434/api/generate\"\n",
        "payload = {\n",
        "    \"model\": \"llama3.2\",\n",
        "    \"prompt\": \"who is the president of USA\",\n",
        "    \"stream\": False\n",
        "}\n",
        "\n",
        "# Set the headers\n",
        "headers = {\n",
        "    'Content-Type': 'application/json'\n",
        "}\n",
        "\n",
        "# Make the POST request\n",
        "response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
        "\n",
        "# Print the response\n",
        "print(response.json())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3030e8d-c55c-4b7e-9643-a4a1c4dedc0c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3030e8d-c55c-4b7e-9643-a4a1c4dedc0c",
        "outputId": "e64621ee-ee11-46ea-b1db-3492866e22b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P\n",
            "To calculate the result of the query \"2 * pi\" using Python's math library, you can use the following code:\n",
            "\n",
            "```\n",
            "import math\n",
            "result = math.pi * 2\n",
            "print(result)\n",
            "```\n",
            "Result: To calculate the result of the query \"2 * pi\" using Python's math library, you can use the following code:\n",
            "\n",
            "```\n",
            "import math\n",
            "result = math.pi * 2\n",
            "print(result)\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "# Assuming you have a function to call your local Llama server\n",
        "class LlamaLLM:\n",
        "    def __init__(self, url):\n",
        "        self.url = url\n",
        "\n",
        "    def generate(self, prompt):\n",
        "        payload = {\n",
        "            \"model\": \"llama3.2\",\n",
        "            \"prompt\": prompt,\n",
        "            \"stream\": False\n",
        "        }\n",
        "        headers = {\n",
        "            'Content-Type': 'application/json'\n",
        "        }\n",
        "        response = requests.post(self.url, headers=headers, data=json.dumps(payload))\n",
        "        response.raise_for_status()\n",
        "        return response.json()['response']\n",
        "\n",
        "# Initialize your Llama model\n",
        "llm = LlamaLLM(url=\"http://localhost:11434/api/generate\")\n",
        "\n",
        "# Define the prompt template\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"query\"],\n",
        "    template=\"{query}\"\n",
        ")\n",
        "\n",
        "# Create a simple function to simulate the LLMChain behavior\n",
        "def llm_chain_run(llm, prompt, query):\n",
        "    formatted_prompt = prompt.format(query=query)\n",
        "    response = llm.generate(formatted_prompt)\n",
        "    print(\"P\")\n",
        "    print(response)\n",
        "    return response\n",
        "\n",
        "# Define a function for the calculator\n",
        "def simple_calculator():\n",
        "    query = \"Calculate 2 * pi\"\n",
        "    result = llm_chain_run(llm, prompt, {\"query\": query})\n",
        "    print(f\"Result: {result}\")\n",
        "\n",
        "# Run the calculator\n",
        "simple_calculator()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20ac7ad7-40cf-4bb1-b4b6-58a38642ac37",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20ac7ad7-40cf-4bb1-b4b6-58a38642ac37",
        "outputId": "ede041ac-b7f0-4f3e-e4cf-fabc717306a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Passed: Result is approximately correct.\n",
            "Result: The value of pi (π) is approximately 3.14159.\n",
            "\n",
            " Multiplying 2 by pi gives:\n",
            "\n",
            "2 × π = 2 × 3.14159 ≈ 6.28318\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Class to interact with the Llama model server\n",
        "class LlamaLLM:\n",
        "    def __init__(self, url):\n",
        "        self.url = url\n",
        "\n",
        "    def generate(self, prompt):\n",
        "        # Prepare the payload for the POST request\n",
        "        payload = {\n",
        "            \"model\": \"llama3.2\",\n",
        "            \"prompt\": prompt,\n",
        "            \"stream\": False\n",
        "        }\n",
        "        headers = {\n",
        "            'Content-Type': 'application/json'\n",
        "        }\n",
        "        # Send the request to the Llama model server\n",
        "        response = requests.post(self.url, headers=headers, data=json.dumps(payload))\n",
        "        response.raise_for_status()\n",
        "        return response.json()['response']\n",
        "\n",
        "# Define the prompt template\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"query\"],\n",
        "    template=\"{query}\"\n",
        ")\n",
        "\n",
        "# Agent class for the simple calculator\n",
        "class SimpleCalculatorAgent:\n",
        "    def __init__(self, llm, prompt):\n",
        "        self.llm = llm\n",
        "        self.prompt = prompt\n",
        "\n",
        "    def run(self, query):\n",
        "        # Format the prompt with the input query\n",
        "        formatted_prompt = self.prompt.format(query=query)\n",
        "        # Generate the response from the Llama model\n",
        "        response = self.llm.generate(formatted_prompt)\n",
        "        return self.validate_response(response)\n",
        "\n",
        "    def validate_response(self, response):\n",
        "        # Validate the response\n",
        "        try:\n",
        "            # Expected value for 2 * pi\n",
        "            expected_value = 2 * 3.14159\n",
        "            # Extract the numerical value from the response\n",
        "            if \"6.28318\" in response:\n",
        "                return \"Validation Passed: Result is approximately correct.\", response\n",
        "            else:\n",
        "                return \"Validation Failed: Result is incorrect.\", response\n",
        "        except Exception as e:\n",
        "            return f\"Error in validating the result: {e}\", response\n",
        "\n",
        "# Initialize the Llama model with the server URL\n",
        "llm = LlamaLLM(url=\"http://localhost:11434/api/generate\")\n",
        "\n",
        "# Create an instance of the SimpleCalculatorAgent\n",
        "calculator_agent = SimpleCalculatorAgent(llm, prompt)\n",
        "\n",
        "# Function to use the agent for calculating\n",
        "def use_calculator_agent():\n",
        "    query = \"Calculate 2 * pi\"\n",
        "    validation_message, result = calculator_agent.run(query)\n",
        "    print(validation_message)\n",
        "    print(f\"Result: {result}\")\n",
        "\n",
        "# Run the calculator agent\n",
        "use_calculator_agent()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f3b2e19-eee7-499d-bbfe-3014db3e5ec7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f3b2e19-eee7-499d-bbfe-3014db3e5ec7",
        "outputId": "1dc3c316-06ff-4bd6-ca40-ffefd1d9c02b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw Llama Response: 2 to the power of 5 (2^5) equals 32.\n",
            "State: initial_state, Action: achieve_goal, Result: 32, Attempts: 1\n",
            "Goal achieved with result: 32\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "import re\n",
        "\n",
        "# Class to interact with the Llama model server\n",
        "class LlamaLLM:\n",
        "    def __init__(self, url):\n",
        "        self.url = url\n",
        "\n",
        "    def generate(self, prompt):\n",
        "        payload = {\n",
        "            \"model\": \"llama3.2\",\n",
        "            \"prompt\": prompt,\n",
        "            \"stream\": False\n",
        "        }\n",
        "        headers = {\n",
        "            'Content-Type': 'application/json'\n",
        "        }\n",
        "        response = requests.post(self.url, headers=headers, data=json.dumps(payload))\n",
        "        response.raise_for_status()\n",
        "        return response.json()['response']\n",
        "\n",
        "# Environment class remains the same\n",
        "class Environment:\n",
        "    def __init__(self):\n",
        "        self.state = 'initial_state'\n",
        "        self.goal_achieved = False\n",
        "        self.result = None\n",
        "\n",
        "    def get_state(self):\n",
        "        return self.state\n",
        "\n",
        "    def change_state(self, result):\n",
        "        self.state = 'goal_state'\n",
        "        self.result = result\n",
        "\n",
        "# ReActAgent class modified to use LlamaLLM for reasoning\n",
        "class ReActAgent:\n",
        "    def __init__(self, environment, llm, max_attempts=5):\n",
        "        self.environment = environment\n",
        "        self.llm = llm  # Llama 3.1 model\n",
        "        self.max_attempts = max_attempts\n",
        "\n",
        "    def perceive(self):\n",
        "        return self.environment.get_state()\n",
        "\n",
        "    def reason(self, state):\n",
        "        prompt = \"What is 2^5?\"\n",
        "        response = self.llm.generate(prompt).strip()\n",
        "        print(f\"Raw Llama Response: {response}\")  # Debugging: print raw response\n",
        "        # Check if the correct result is in the response\n",
        "        if '32' in response:\n",
        "            return 'achieve_goal', 32\n",
        "        return 'take_action', None\n",
        "\n",
        "    def act(self, action, result):\n",
        "        if action == 'achieve_goal':\n",
        "            self.environment.goal_achieved = True\n",
        "            self.environment.change_state(result)\n",
        "\n",
        "# Initialize the Llama model with the server URL\n",
        "llm = LlamaLLM(url=\"http://localhost:11434/api/generate\")\n",
        "\n",
        "# Create environment and agent\n",
        "env = Environment()\n",
        "agent = ReActAgent(env, llm)\n",
        "\n",
        "# Agent perception, reasoning, and action loop\n",
        "attempts = 0\n",
        "while not env.goal_achieved and attempts < agent.max_attempts:\n",
        "    state = agent.perceive()\n",
        "    action, result = agent.reason(state)\n",
        "    agent.act(action, result)\n",
        "    attempts += 1\n",
        "    print(f\"State: {state}, Action: {action}, Result: {result}, Attempts: {attempts}\")\n",
        "\n",
        "if env.goal_achieved:\n",
        "    print(f\"Goal achieved with result: {env.result}\")\n",
        "else:\n",
        "    print(\"Failed to achieve goal within the maximum number of attempts.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e205a953-5558-442d-9c1a-1eabb5a15c8a",
      "metadata": {
        "id": "e205a953-5558-442d-9c1a-1eabb5a15c8a"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}